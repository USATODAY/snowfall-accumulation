))
# View the summary
#view(summary)
#session resets
sessions <- logs_df2 %>%
filter (str_detect(case_num, "Name: ASP.NET_SessionId, Value: ")) %>%
mutate (session_id = str_remove_all(case_num, "Name: ASP.NET_SessionId, Value: "))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp)) %>%
select (-case_num)
#recache times
recaching <- logs_df2 %>%
filter (str_detect(case_num, "^Cache expired"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
summary2 <- summary %>%
filter (date == Sys.Date()) %>%
arrange (comp)
view(summary2)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary4 <- summary3 %>% group_by (date) %>% summarize (total=sum(cases))
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2004 cases:", summary$min_case[1]-448074))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
#how many per year?
years <- c(2001, 2004:2017)
#records <- lapply(years, function(x){
#  records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", x, "/", x, "_output_cases.csv"))
#})
#records2 <- data.table::rbindlist(records) %>%
#  mutate (year = substr(case_number, 4,5))
#dispositions <- records2 %>% count (disposition, sort=T)
#record_counts <- records2 %>%
#  group_by (year) %>%
#  summarize (count = n_distinct (case_number),
#             rape = n_distinct (str_detect(charge_description, "SEX|RAPE")))
record_counts <- lapply(years, function(x){
records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", x, "/", x, "_output_cases.csv"))
count <- records %>%
distinct (case_number) %>%
nrow()
rape_count <- records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL")) %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha) %>%
distinct (num) %>%
nrow()
min <- substr(min(records$case_number, na.rm=T), 7, 12)
max <- substr(max(records$case_number, na.rm=T), 7, 12)
range <- paste(substr(min(records$case_number, na.rm=T), 7, 12), "-", substr(max(records$case_number, na.rm=T), 7, 12))
this_row <- tibble(year=x, cases=count, rape=rape_count, case_range = range)
})
record_counts2 <- data.table::rbindlist(record_counts) %>%
adorn_totals() %>%
mutate (pct = round(rape/cases*100,2)) %>%
select (1:3, pct, range)
View(record_counts)
record_counts2 <- data.table::rbindlist(record_counts) %>%
adorn_totals() %>%
mutate (pct = round(rape/cases*100,2)) %>%
select (1:3, pct, case_range)
View(record_counts2)
head(summary2$min_case[summary2$comp=="pers"],1)
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-448074)
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-448074))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-448074))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
json <- jsonlite::fromJSON("/Users/AFast/Documents/cleveland_smith_output.json") %>%
clean_names() %>%
mutate (filing_date = mdy(filing_date),
case_num = substr(case_number, 7,12),
year = paste0(if_else(substr(case_number, 4,5) < 26, "20", "19"), substr(case_number, 4,5)))
json2 <- json %>%
group_by (year) %>%
summarize (min = min(case_num),
min_date = filing_date[which.min(case_num)],
max = max(case_num),
max_date = filing_date[which.max(case_num)]) %>%
arrange (desc(year))
View(json2)
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
summary2 <- summary %>%
filter (date == Sys.Date()) %>%
arrange (comp)
view(summary2)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary4 <- summary3 %>% group_by (date) %>% summarize (total=sum(cases))
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
#how many per year?
years <- c(2001, 2003:2017)
#records <- lapply(years, function(x){
#  records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", x, "/", x, "_output_cases.csv"))
#})
#records2 <- data.table::rbindlist(records) %>%
#  mutate (year = substr(case_number, 4,5))
#dispositions <- records2 %>% count (disposition, sort=T)
#record_counts <- records2 %>%
#  group_by (year) %>%
#  summarize (count = n_distinct (case_number),
#             rape = n_distinct (str_detect(charge_description, "SEX|RAPE")))
record_counts <- lapply(years, function(x){
records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", x, "/", x, "_output_cases.csv"))
count <- records %>%
distinct (case_number) %>%
nrow()
rape_count <- records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL")) %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha) %>%
distinct (num) %>%
nrow()
min <- substr(min(records$case_number, na.rm=T), 7, 12)
max <- substr(max(records$case_number, na.rm=T), 7, 12)
range <- paste(substr(min(records$case_number, na.rm=T), 7, 12), "-", substr(max(records$case_number, na.rm=T), 7, 12))
this_row <- tibble(year=x, cases=count, rape=rape_count, case_range = range)
})
record_counts2 <- data.table::rbindlist(record_counts) %>%
adorn_totals() %>%
mutate (pct = round(rape/cases*100,2)) %>%
select (1:3, pct, case_range)
#how many per year?
years <- c(2001:2017)
#records <- lapply(years, function(x){
#  records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", x, "/", x, "_output_cases.csv"))
#})
#records2 <- data.table::rbindlist(records) %>%
#  mutate (year = substr(case_number, 4,5))
#dispositions <- records2 %>% count (disposition, sort=T)
#record_counts <- records2 %>%
#  group_by (year) %>%
#  summarize (count = n_distinct (case_number),
#             rape = n_distinct (str_detect(charge_description, "SEX|RAPE")))
record_counts <- lapply(years, function(x){
records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", x, "/", x, "_output_cases.csv"))
count <- records %>%
distinct (case_number) %>%
nrow()
rape_count <- records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL")) %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha) %>%
distinct (num) %>%
nrow()
min <- substr(min(records$case_number, na.rm=T), 7, 12)
max <- substr(max(records$case_number, na.rm=T), 7, 12)
range <- paste(substr(min(records$case_number, na.rm=T), 7, 12), "-", substr(max(records$case_number, na.rm=T), 7, 12))
this_row <- tibble(year=x, cases=count, rape=rape_count, case_range = range)
})
record_counts2 <- data.table::rbindlist(record_counts) %>%
adorn_totals() %>%
mutate (pct = round(rape/cases*100,2)) %>%
select (1:3, pct, case_range)
View(record_counts2)
432383 - 448074
429221 - 433158
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(tesseract)
library(lubridate)
#library(tidyr) #Can remove this with drop_na gone?
library(jsonlite)
library(tibble)
#SOURCE: https://www.nohrsc.noaa.gov/snowfall/
#Set time zone to pull files using Eastern time so GitHub Actions doesn't use UTC.
#Sys.setenv(TZ="America/New_York")
Sys.setenv(TZ="UTC")
# Form path to URL: First, check the time and output 12 or 00 -- use 12 after 1 p.m. UTC. Otherwise, use 00.
hour <- if (as.numeric(format(Sys.time(), "%H")) >= 13 && as.numeric(format(Sys.time(), "%H")) < 24) {
"12"
} else {
"00"
}
# first get current year
current_year <- as.numeric(format(Sys.Date(), "%Y"))
# determine the start of the season (oct 1 of the current or previous year)
season_start_year <- if (format(Sys.Date(), "%m") < "10") {
current_year - 1  # ff it's before oct, use the previous year
} else {
current_year  # otherwise, use the current year
}
season_start <- paste0(season_start_year, "093012")  # october 1, 12 UTC
# define the end of the season as the current date
season_end <- paste0(format(Sys.Date(), "%Y%m%d"), "12") # hour fixed, no "00" files
# connect all this for seasonal URL
path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date(), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_", season_end, ".tif")
# load the raster
#r <- rast(path_to_raster)
r <- try(rast(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one day behind the version just tried (1/28/25 instead of 1/29/25, etc.)
new_path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date()-days(1), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_",
format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
r <- rast(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12")
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", season_end)
}
# Read raster with {stars}
r2 <- read_stars(new_path_to_raster)
#TRYING NEW CODE HERE
library(stars)
# Read raster with {stars}
r2 <- read_stars(new_path_to_raster)
# Convert raster to polygons
r_poly2 <- st_as_sf(r2, merge = TRUE)  # Merge adjacent areas of same value
r_poly3 <- r_poly2 %>%
mutate (group = case_when(
sfav2_CONUS_2024093012_to_2025012912.tif < 0 ~ -1,
sfav2_CONUS_2024093012_to_2025012912.tif == 0 ~ 0,
between(sfav2_CONUS_2024093012_to_2025012912.tif, 0, 1) ~ 1,
TRUE ~ round(sfav2_CONUS_2024093012_to_2025012912.tif)))
breaks4 <- c(-0.01, 0, 1, 2, 4, 6, 8, seq(12, 60, by=6), seq(72, 180, by=12), seq(240, max(r_poly_sf[[column_name]])+60, by=60))
breaks4 <- c(-0.01, 0, 1, 2, 4, 6, 8, seq(12, 60, by=6), seq(72, 180, by=12), seq(240, max(r_poly3[[column_name]])+60, by=60))
labels4 <- breaks4[2:length(breaks4)]
r_poly3$color_factor <- cut(r_poly3[[paste0(column_name, ".tif")]],
breaks = breaks4,
labels = labels4,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
breaks4 <- c(-0.01, 0, 1, 2, 4, 6, 8, seq(12, 60, by=6), seq(72, 180, by=12), seq(240, max(r_poly3[[column_name]])+60, by=60))
breaks4 <- c(-0.01, 0, 1, 2, 4, 6, 8, seq(12, 60, by=6), seq(72, 180, by=12), seq(240, max(r_poly3[[paste0(column_name, ".tif")]])+60, by=60))
labels4 <- breaks4[2:length(breaks4)]
r_poly3$color_factor <- cut(r_poly3[[paste0(column_name, ".tif")]],
breaks = breaks4,
labels = labels4,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
check <- r_poly3 %>% st_drop_geometry() %>% count (color_factor)
View(check)
system.time(#153s
r_poly4 <- r_poly3 %>%
group_by (color_factor) %>%
summarize (geometry = st_union (geometry)) %>%
#filter (color_factor != 0) %>%
ungroup() %>%
mutate (accumulation = as.numeric (as.character(color_factor))) %>%
select (accumulation) %>%
filter (accumulation > 0) #remove 0 and NAs
)
check2b <- r_poly4 %>% st_drop_geometry()
r_poly6 <- r_poly4 %>%
st_transform(2163) %>%  # Convert to meters for spatial operations
st_buffer(0)
r_poly7 <- r_poly6 %>%
st_simplify(dTolerance = 50)  # Round coordinates to nearest 100 meters
geojson_data <- geojsonio::geojson_json(r_poly7)
st_crs(r_poly4)
r_poly7 <- r_poly6 %>%
st_simplify(dTolerance = 50) %>% # Round coordinates to nearest 100 meters
st_transform(4326)
geojson_data <- geojsonio::geojson_json(r_poly7)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_newest_stars-breaks4-simpbuff.json"),
object_name = "snowfall",
#precision = 0,
#quantization = 1e3,
overwrite = TRUE)
setwd("~/Documents/GitHub/snowfall-accumulation")
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_newest_stars-breaks4-simpbuff.json"),
object_name = "snowfall",
#precision = 0,
#quantization = 1e3,
overwrite = TRUE)
r_poly7 <- r_poly6 %>%
st_simplify(dTolerance = 200) %>% # Round coordinates to nearest 100 meters
st_transform(4326)
geojson_data <- geojsonio::geojson_json(r_poly7)
r_poly7 <- r_poly6 %>%
st_simplify(dTolerance = 500) %>% # Round coordinates to nearest 100 meters
st_transform(4326)
geojson_data <- geojsonio::geojson_json(r_poly7)
r_poly7 <- r_poly6 %>%
st_simplify(dTolerance = 1000) %>% # Round coordinates to nearest 100 meters
st_transform(4326)
geojson_data <- geojsonio::geojson_json(r_poly7)
# Function to count vertices
count_vertices <- function(geom) {
sum(sapply(st_geometry(geom), function(g) length(st_coordinates(g))))
}
# Count vertices before transformation
vertices_before <- count_vertices(r_poly4)
# Count vertices after transformation
vertices_after <- count_vertices(r_poly6)
# Print results
cat("Vertices before:", vertices_before, "\n")
cat("Vertices after:", vertices_after, "\n")
# Count vertices before transformation
vertices_before <- count_vertices(r_poly3)
# Count vertices before transformation
vertices_before <- count_vertices(r_poly3)
# Count vertices before transformation
vertices_before <- count_vertices(r_poly4)
# Count vertices after transformation
vertices_after <- count_vertices(r_poly6)
check <0 r_poly3 %>% st_drop_geometry() %>% count (group)
check <- r_poly3 %>% st_drop_geometry() %>% count (group)
View(check)
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
sfav2_CONUS_2024093012_to_2025012912.tif < 0 ~ -1,
sfav2_CONUS_2024093012_to_2025012912.tif == 0 ~ 0,
between(sfav2_CONUS_2024093012_to_2025012912.tif, 0, 1) ~ 1,
TRUE ~ round(sfav2_CONUS_2024093012_to_2025012912.tif)))
r_poly4 <- r_poly3 %>%
group_by (accumulation) %>%
summarize (geometry=st_union(geometry)) %>%
filter (accumulation > 0)
check3 <- r_poly4 %>% st_drop_geometry()
View(check3)
r_poly5 <- r_poly4%>%
st_make_valid() %>%
st_simplify(dTolerance = 10)
#stars data=MB
geojson_data <- geojsonio::geojson_json(r_poly5)
breaks4 <- c(-0.01, 0, 1, 2, 4, 6, 8, seq(12, 60, by=6), seq(72, 180, by=12), seq(240, max(r_poly3[[paste0(column_name, ".tif")]])+60, by=60))
breaks4
180*12
180/12
480/12
View(check2b)
check3 <- r_poly4 %>% st_drop_geometry()
View(check3)
breaks5 <- c(-0.01, 0, 0.1, 1, 2, 3, 6,
1*12, 2*12, 3*12, 10*12, 20*12,
1000)
labels5 <- breaks5[2:length(breaks5)]
r_poly3$color_factor <- cut(r_poly3[[paste0(column_name, ".tif")]],
breaks = breaks5,
labels = labels5,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
check <- r_poly3 %>% st_drop_geometry() %>% count (color_factor)
View(check)
breaks4 <- c(-0.01, 0, 0.1, 1, 2, 4, 6, 8, seq(12, 60, by=6), seq(72, 180, by=12), seq(240, max(r_poly3[[paste0(column_name, ".tif")]])+60, by=60))
labels4 <- breaks4[2:length(breaks4)]
r_poly3$color_factor <- cut(r_poly3[[paste0(column_name, ".tif")]],
breaks = breaks4,
labels = labels4,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
check <- r_poly3 %>% st_drop_geometry() %>% count (color_factor)
View(check)
breaks5 <- c(-0.01, 0, 0.1, 1, 2, 3, 6,
seq(12, 120, by=12),
seq(240, max(r_poly3[[paste0(column_name, ".tif")]])+60, 1000)
labels5 <- breaks5[2:length(breaks5)]
breaks5 <- c(-0.01, 0, 0.1, 1, 2, 3, 6,
seq(12, 120, by=12),
seq(240, max(r_poly3[[paste0(column_name, ".tif")]])+60),
1000)
120/12
12*15
breaks5 <- c(-0.01, 0, 0.1, 1, 2, 3, 6,
seq(12, 120, by=12),
seq(180, max(r_poly3[[paste0(column_name, ".tif")]])+60),
1000)
max(r_poly3$sfav2_CONUS_2024093012_to_2025012912.tif)
breaks5 <- c(-0.01, 0, 0.1, 1, 2, 3, 6,
seq(12, 120, by=12),
seq(180, max(r_poly3[[paste0(column_name, ".tif")]])+60, by=60),
1000)
labels5 <- breaks5[2:length(breaks5)]
r_poly3$color_factor <- cut(r_poly3[[paste0(column_name, ".tif")]],
breaks = breaks5,
labels = labels5,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
check <- r_poly3 %>% st_drop_geometry() %>% count (color_factor)
View(check)
breaks5 <- c(-0.01, 0, 0.1, 1, 2, 3, 6, 12, 18,
seq(24, 120, by=12),
seq(180, max(r_poly3[[paste0(column_name, ".tif")]])+60, by=60),
1000)
labels5 <- breaks5[2:length(breaks5)]
r_poly3$color_factor <- cut(r_poly3[[paste0(column_name, ".tif")]],
breaks = breaks5,
labels = labels5,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
check <- r_poly3 %>% st_drop_geometry() %>% count (color_factor)
View(check)
system.time(#153s
r_poly4 <- r_poly3 %>%
group_by (color_factor) %>%
summarize (geometry = st_union (geometry)) %>%
#filter (color_factor != 0) %>%
ungroup() %>%
mutate (accumulation = as.numeric (as.character(color_factor))) %>%
select (accumulation) %>%
filter (accumulation > 0) #remove 0 and NAs
)
check2b <- r_poly4 %>% st_drop_geometry()
r_poly5 <- r_poly4 %>%
st_make_valid() %>%
st_simplify(dTolerance = 10)
geojson_data <- geojsonio::geojson_json(r_poly5)
View(check2b)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_newest_stars-breaks5.json"),
object_name = "snowfall",
#precision = 0,
#quantization = 1e3,
overwrite = TRUE)
View(check3)
View(check2b)
View(check)
