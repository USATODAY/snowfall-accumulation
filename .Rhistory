mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
range(court_records$case_number)
#How many cases have been scraped for this year?
as.numeric(substr(max(court_records$case_number), 7, 12))-as.numeric(substr(min(court_records$case_number), 7, 12))
#How many cases, including those with multiples?
n_distinct(court_records$case_number)
court_records %>% count (disposition, sort=T)
charges <- court_records %>%
count (charge_description, sort=T)
court_records %>% count (def_status, sort=T)
court_records %>%
mutate (case_num = substr(case_number, 7, 12)) %>%
summarize (cases_found = n_distinct(case_num),
i_range = range (case_num))
#find sexual assault/rape cases
rape <- court_records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL"))
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (charge_description, sort = T)
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (disposition, sort = T)
#pull these dockets
rape2 <- rape %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha)
#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
if (year >= 2004){
indict <- joined %>%
filter (str_detect(event_description, "INDICT")) %>%
mutate (event_date = mdy(event_date)) %>%
arrange (event_date) %>%
select (event_date, everything())
#  summarize (indict_dates = range(event_date))
arrest <- joined %>%
filter (str_detect(event_description, "ARREST"))  %>%
mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
} else {
indict <- joined %>%
filter (str_detect(event_description, "BINDOVER")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ORIGINAL"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date),
count = n())
}
year = "2001"
#court_records <- read_csv ("/Users/AFast/Documents/python/output_cases.csv")
#court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/250104_output_cases.csv")
court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_cases.csv")
case_actions <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_case_actions.csv")
case_actions %>% count (event_description, sort=T)
court_records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_cases.csv"))
case_actions <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_case_actions.csv"))
joined <- full_join (court_records, case_actions, by=c("case_number"="case_num"))
if (year >= 2004){
indict <- joined %>%
filter (str_detect(event_description, "INDICT")) %>%
mutate (event_date = mdy(event_date)) %>%
arrange (event_date) %>%
select (event_date, everything())
#  summarize (indict_dates = range(event_date))
arrest <- joined %>%
filter (str_detect(event_description, "ARREST"))  %>%
mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
} else {
indict <- joined %>%
filter (str_detect(event_description, "BINDOVER")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ORIGINAL"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date),
count = n())
}
range(court_records$case_number)
#How many cases have been scraped for this year?
as.numeric(substr(max(court_records$case_number), 7, 12))-as.numeric(substr(min(court_records$case_number), 7, 12))
#How many cases, including those with multiples?
n_distinct(court_records$case_number)
court_records %>% count (disposition, sort=T)
charges <- court_records %>%
count (charge_description, sort=T)
court_records %>% count (def_status, sort=T)
court_records %>%
mutate (case_num = substr(case_number, 7, 12)) %>%
summarize (cases_found = n_distinct(case_num),
i_range = range (case_num))
#find sexual assault/rape cases
rape <- court_records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL"))
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (charge_description, sort = T)
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (disposition, sort = T)
#pull these dockets
rape2 <- rape %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha)
#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
#Set time zone to pull files using Eastern time so GitHub Actions doesn't use UTC.
Sys.setenv(TZ="America/New_York")
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(tesseract)
library(lubridate)
#library(tidyr) #Can remove this with drop_na gone?
library(jsonlite)
library(tibble)
#Set time zone to pull files using Eastern time so GitHub Actions doesn't use UTC.
#Sys.setenv(TZ="America/New_York")
Sys.setenv(TZ="UTC")
# Form path to URL: First, check the time and output 12 or 00 -- use 12 after 1 p.m. UTC. Otherwise, use 00.
hour <- if (as.numeric(format(Sys.time(), "%H")) >= 13 && as.numeric(format(Sys.time(), "%H")) < 24) {
"12"
} else {
"00"
}
timeframes <- c("24h_", "48h_", "72h_")
# we'll make a function to get and process the latest raster
raster2vector <- function(timeframe){
#timeframe <- "72h_"
# make a url
path_to_raster <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/", format(Sys.Date(), "%Y%m"), "/sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour, ".tif")
# load the raster
#r <- rast(path_to_raster)
r <- try(rast(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one version behind (if tried 12 UTC for 1/29/25, create path for 00 UTC 1/29/25, etc.)
new_path_to_raster <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/",
if_else (hour==12, format(Sys.Date(), "%Y%m"), format(Sys.Date()-days(1), "%Y%m")),
"/sfav2_CONUS_", timeframe,
if_else (hour==12, format(Sys.Date(), "%Y%m%d"), format(Sys.Date()-days(1), "%Y%m%d")),
if_else (hour==12, "00", "12"), ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 29, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 29, nchar(new_path_to_raster))))
r <- rast(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 29, nchar(new_path_to_raster))))
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 29, nchar(path_to_raster))))
}
# round raster values to 1 decimal place to capture those between 0-0.1, or else they'll all be rounded to 0 since as.polygons() outputs the nearest integer
r_rounded <- round(r, 1)*10
# convert to polygons with rounded raster values
r_poly <- as.polygons(r_rounded)
# convert raster polygons to sf
r_poly_sf <- st_as_sf(r_poly)
# define the breaks and colors
breaks <- c(-0.01, 0, 0.1, 1, 2,
3, 4, 6, 8, 12,
18, 24, 30, 36, 48,
60, 72, 96, 120, 500)  # added -0.01 and 500 to handle zero + >120 explicitly
# multiply by 10 to match the rounding done above.
breaks = breaks*10
# used https://imagecolorpicker.com/ with the png file to set these 19 colors based on NOAA scale.
colors <- c("#ffffff", "#e4eef4", "#bdd7e7", "#6bafd6", "#2d83be",
"#02509d", "#022195", "#fefe96", "#ffc500", "#ff8800",
"#dc0c00", "#9f0000", "#690000", "#330000", "#cdcdff",
"#a08dd9", "#7d51a6", "#551573", "#290030")
# define the corresponding labels (numeric values)
labels <- c("0", "0.1", "1", "2", "3",
"4", "6", "8", "12", "18",
"24", "30", "36", "48", "60",
"72", "96", "120", ">120")
# form what the column name should be
column_name <- paste0("sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour)
# assign color categories based on the breaks
r_poly_sf$color_factor <- cut(r_poly_sf[[column_name]],
breaks = breaks,
labels = labels,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
# now assign colors to the color_factor
r_poly_sf$colors <- colors[as.numeric(r_poly_sf$color_factor)]
# spatial join all the 0.1 values between breaks together. This will reduce from hundreds of polygons to <=18 categories.
r_poly_sf2 <- r_poly_sf %>%
#group_by (color_factor, colors) %>%
#summarize (geometry = st_union (geometry)) %>%
#drop_na() %>%
mutate (accumulation = r_poly_sf[[column_name]]/10) %>%
filter (accumulation > 0) %>%
select (accumulation)
#    select (accumulation, color_factor) %>%
#    filter (color_factor != 0)
#ungroup() %>%
#rename (hue = colors) %>%
#select (color_factor)
return(r_poly_sf2)
}
current_year <- as.numeric(format(Sys.Date(), "%Y"))
# determine the start of the season (oct 1 of the current or previous year)
season_start_year <- if (format(Sys.Date(), "%m") < "10") {
current_year - 1  # ff it's before oct, use the previous year
} else {
current_year  # otherwise, use the current year
}
season_start <- paste0(season_start_year, "093012")  # october 1, 12 UTC
# define the end of the season as the current date
season_end <- paste0(format(Sys.Date(), "%Y%m%d"), "12") # hour fixed, no "00" files
# connect all this for seasonal URL
path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date(), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_", season_end, ".tif")
# load the raster
#r <- rast(path_to_raster)
r <- try(rast(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one day behind the version just tried (1/28/25 instead of 1/29/25, etc.)
new_path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date()-days(1), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_",
format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
r <- rast(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12")
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", season_end)
}
#CLEAN RUN
library(stars)
# Read raster with {stars}
r2 <- read_stars(path_to_raster)
r_poly2 <- st_as_sf(r2, merge = TRUE)  # Merge adjacent areas of same value
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
get(column_name) < 0 ~ -1,
get(column_name) == 0 ~ 0,
between(get(column_name), 0, 1) ~ 1,
TRUE ~ round(get(column_name))))
names(r_poly2)
# Read raster with {stars}
column_name <- paste0(column_name, ".tif")
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
get(column_name) < 0 ~ -1,
get(column_name) == 0 ~ 0,
between(get(column_name), 0, 1) ~ 1,
TRUE ~ round(get(column_name))))
check <- r_poly3 %>% st_drop_geometry()
View(check)
range(check$accumulation)
system.time(
r_poly4 <- r_poly3 %>%
filter (accumulation > 0) %>%
group_by (accumulation) %>%
summarize (geometry=st_union(geometry))
)
r_poly5 <- r_poly4 %>%
st_make_valid()
geojsonio::topojson_write(r_poly5,
file = "outputs/test/snowfall-full.json",
object_name = "snowfall",
overwrite = TRUE)
setwd("~/Documents/GitHub/snowfall-accumulation")
geojsonio::topojson_write(r_poly5,
file = "outputs/test/snowfall-full.json",
object_name = "snowfall",
overwrite = TRUE)
file1 <- "outputs/test/snowfall-full.json"
file2 <- "outputs/test/snowfall-full-topo.json"
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file1, "-o precision=0.1", file2))
squeezed <- geojsonio::topojson_read(file2)
check2 <- squeezed %>% st_drop_geometry()
missing <- anti_join (check, squeezed)
#remove slivers on edges
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 0.01)
file2 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
file2 <- "outputs/test/snowfall-full-topo.json"
file3 <- "outputs/test/snowfall-full-topo2.json"
check <- r_poly3 %>% st_drop_geometry()
check <- r_poly4 %>% st_drop_geometry()
check <- r_poly4 %>% st_drop_geometry()
check2 <- squeezed %>% st_drop_geometry()
missing <- anti_join (check, squeezed)
View(missing)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "precision=0.1", "no-quantization", file2))
squeezed <- geojsonio::topojson_read(file2)
check2 <- squeezed %>% st_drop_geometry()
missing <- anti_join (check, squeezed)
#remove slivers on edges
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 0.01)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "precision=0.01", "no-quantization", file2))
squeezed <- geojsonio::topojson_read(file2)
check2 <- squeezed %>% st_drop_geometry()
missing <- anti_join (check, squeezed)
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 0.01)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
#remove slivers on edges
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 10)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
#remove slivers on edges
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 1)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
#remove slivers on edges
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 0.1)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
#remove slivers on edges
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 0.01)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "quantization=100000", file2))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "quantization=10000", file2))
squeezed <- geojsonio::topojson_read(file2)
check2 <- squeezed %>% st_drop_geometry()
missing <- anti_join (check, squeezed)
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "quantization=1000", file2))
squeezed <- geojsonio::topojson_read(file2)
check2 <- squeezed %>% st_drop_geometry()
missing <- anti_join (check, squeezed)
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 0.01)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "quantization=100", file2))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "quantization=500", file2))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file1, "-o precision=0.1", file2))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "quantization=1000000", file2))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file1, "-o precision=0.1", file2))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper",
file1,
"-o", "precision=0.01", "no-quantization", file2))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file1, "-o precision=0.1", file2))
file3 <- "outputs/test/snowfall-full-topo2.json"
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-simplify 0.01 -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-simplify 0.05 -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-simplify 0.5 -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-clean -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-dissolve -clean -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-simplify 0.1 keep-shapes -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-simplify 0.5 keep-shapes -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-simplify 0.75 keep-shapes -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-clean -o", file3))
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-clean -verbose -o", file3))
squeezed <- geojsonio::topojson_read(file2)
check2 <- squeezed %>% st_drop_geometry()
missing <- anti_join (check, squeezed)
#remove slivers on edges
squeezed2 <- squeezed %>%
#st_make_valid() %>%
st_simplify(dTolerance = 0.01)
file3 <- "outputs/test/snowfall-full-topo2.json"
geojsonio::topojson_write(squeezed2,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
file3 <- "outputs/test/snowfall-full-topo3.json"
system(paste("/Users/AFast/.nvm/versions/node/v22.11.0/bin/mapshaper", file2, "-clean -o", file3))
intersections = st_intersects(x = squeezed2, y = squeezed2)
intersections_log = lengths(intersections) > 0
squeezed3 = squeezed2[intersections_log, ]
