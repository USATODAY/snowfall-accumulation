logs_df2 <- logs_df %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note"),
extra = "drop",
fill = "right") %>%
filter (row_number()>16)
#orig
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
#keeping mult results
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration))
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
case_list <- logs_df3 %>%
distinct (case_num)
# Generate the full sequence from min to max
full_sequence <- seq(min(logs_df3$case_num, na.rm = TRUE), max(logs_df3$case_num, na.rm = TRUE))
# Find missing numbers in log
missing_numbers <- setdiff(full_sequence, logs_df3$case_num)
# Find missing numbers in court records
full_seq <- seq(min(substr(court_records$case_number, 7,12), na.rm = TRUE), max(substr(court_records$case_number, 7,12), na.rm = TRUE))
no_case_records <- setdiff(full_seq, substr(court_records$case_number, 7,12)) %>%
as_tibble()
#Court records saved fine.
check <- no_case_records %>%
count (substr(value, 1,4))
full_seq2 <- seq(min(substr(case_actions$case_num, 7,12), na.rm = TRUE), max(substr(case_actions$case_num, 7,12), na.rm = TRUE))
no_action_records <- setdiff(full_seq2, substr(case_actions$case_num, 7,12)) %>%
as_tibble()
#Case actions for cases 620800-624500 are missing because of typo in code.
check2 <- no_action_records %>%
count (substr(value, 1,4))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE"))
# Add the start and end indices to the breaks for completeness
#breaks <- c(0, breaks, nrow(logs_df3) + 1)
#manual break for 1/10 redo
which (as.character(logs_df3$timestamp)=="2025-01-10 01:55:08")
which (as.character(logs_df3$timestamp)=="2025-01-10 02:26:35")
which (as.character(logs_df3$timestamp)=="2025-01-10 05:37:02")
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, nrow(logs_df3) + 1)
breaks <- sort(breaks)
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
#Count calls on each IP address
logs_df3a <- logs_df2 %>%
filter (str_detect (case_num, "^\\d|Success")|str_detect(str_squish(case_num), "^Process"))
#where did I reset VPN?
breaks2 <- which(logs_df3a$note=="ERROR TERMS OF SERVICE")
#starting after break 6 (7749)
logs_df3b <- logs_df3a %>%
filter (row_number() > breaks2[6]) %>%
filter (str_detect (case_num, "^Success")) %>%
mutate (proxy = str_remove_all(str_remove_all(case_num, "[^0-9.:]"), "^:+")) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
arrange (proxy, timestamp) %>%
group_by (proxy) %>%
mutate (duration = timestamp - lag(timestamp))
ip_count <- logs_df3b %>%
group_by (proxy) %>%
summarize (count = n(),
med = median (duration, na.rm=T),
mean = mean (duration, na.rm=T),
recent = max(timestamp)) %>%
arrange (desc(count))
#keeping mult results
ip_count <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration))
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
ip_count <- logs_df %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip"),
#extra = "drop",
fill = "right") %>%
#keep since i started tracking Ip address
filter (ymd_hms(substr(timestamp,1,19)) > ymd_hms("2025-01-12 11:14:25")) %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "^Name: ASP.NET_SessionId")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num"))
#view(ip_count)
# Function to calculate the total request pulls, case num range, durations, and timestamp min/max
calculate_summary <- function(set) {
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow())  # Add 1 for each additional case result
case_range <- range(set$case_num, na.rm = TRUE)
case_diff <- diff(case_range)  # Calculate the difference between max and min
# Calculate the min and max of timestamp
timestamp_min <- min(set$timestamp, na.rm = TRUE)
timestamp_max <- max(set$timestamp, na.rm = TRUE)
durations <- list(
median_duration = median(set$duration, na.rm = TRUE),
mean_duration = round(mean(set$duration, na.rm = TRUE), 2)
)
tibble(
pulls = pulls,
min_case = case_range[1],
max_case = case_range[2],
diff = case_diff,
median = durations$median_duration,
mean = durations$mean_duration,
start_timestamp = timestamp_min,
end_timestamp = timestamp_max
)
}
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds)
# View the summary
#view(summary)
#this will be slightly higher than count in total_pulls because it includes re-starting sessionIds.
sum(ip_count$count)
#session resets
sessions <- logs_df2 %>%
filter (str_detect(case_num, "Name: ASP.NET_SessionId, Value: ")) %>%
mutate (session_id = str_remove_all(case_num, "Name: ASP.NET_SessionId, Value: "))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp)) %>%
select (-case_num)
#after switch to Proxy()
logs_df3 %>%
filter (row_number()>8955) %>%
summarize (med = median (duration, na.rm=T),
mean = mean (duration, na.rm=T)) %>%
mutate (cases_per_hr = 3600/as.numeric(med))
#recache times
recaching <- logs_df2 %>%
filter (str_detect(case_num, "^Cache expired"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
ip_crosswalk <- tibble(ip="134.238.200.155", gateway="US West", date="1/12/25", set="14") %>%
add_row(ip="208.127.87.65", gateway="US Southwest", date="1/12/25", set="15") %>%
add_row(ip="134.238.14.19", gateway="India West", date="1/12/25", set="16") %>%
add_row(ip="134.238.39.64", gateway="Australia Southeast", date="1/12/25", set="17") %>%
add_row(ip="208.127.73.48", gateway="US East", date="1/13/25", set="18") %>%
add_row(ip="208.127.67.72", gateway="US East", date="1/13/25", set="19") %>%
add_row(ip="165.1.204.206", gateway="US Northeast", date="1/13/25", set="20") %>% #blocked
add_row(ip="208.127.73.48", gateway="US East", date="1/13/25", set="21") %>%
add_row(ip="165.1.204.207", gateway="US Northeast", date="1/13/25", set="22") %>%
add_row(ip="165.1.204.207", gateway="US Northeast", date="1/13/25", set="23") %>%
add_row(ip="208.127.67.72", gateway="US East", date="1/13/25", set="24")%>%
add_row(ip="130.41.15.98", gateway="??", date="1/14/25", set="25")
ip_extra <- logs_df %>%
filter (row_number() > 47744) %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip", "gateway"),
extra = "drop",
fill = "right") %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: "))%>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (case_num = as.numeric(case_num),
date = ymd(substr(timestamp, 1, 10))) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down") %>%
group_by (ip, gateway, date) %>%
summarize (min_case = min(case_num),
max_case = max(case_num),
min_time = min(timestamp),
max_time = max(timestamp)) %>%
mutate (ip = str_remove_all (ip, "IP:"),
gateway = str_remove_all (gateway, "GW:")) %>%
#filter (!is.na(ip)) %>%
select (ip, gateway, min_case, date)
summary2 <- summary %>%
rownames_to_column("set") %>%
full_join (ip_crosswalk, by="set") %>%
mutate (set = as.numeric(set)) %>%
full_join (ip_extra, by="min_case") %>%
mutate (date.x = ymd(date.x)) %>%
mutate (date = coalesce (date.x, date.y),
ip = coalesce (ip.x, ip.y),
gateway = coalesce (gateway.x, gateway.y)) %>%
arrange (desc(set)) %>%
select (-c(gateway.x, gateway.y,ip.x, ip.y,date.x, date.y))
#att 9:40 1/13/25: 166.199.98.115
#view(summary2)
view(summary2)
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
View(all_attachments)
pfas <- readJSON ("/Users/AFast/Documents/250115-PFAS-attendees.json")
pfas <- jsonlite::readJSON ("/Users/AFast/Documents/250115-PFAS-attendees.json")
pfas <- jsonlite::fromJSON ("/Users/AFast/Documents/250115-PFAS-attendees.json")
pfas <- jsonlite::fromJSON ("/Users/AFast/Documents/250115-PFAS-attendees.json") %>%
as_tibble()
View(pfas)
pfas <- jsonlite::fromJSON ("/Users/AFast/Documents/250115-PFAS-attendees.json") %>%
as_tibble("name")
pfas <- jsonlite::fromJSON ("/Users/AFast/Documents/250115-PFAS-attendees.json") %>%
as_tibble() %>%
magrittr::set_names("name")
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
year = "2015"
#court_records <- read_csv ("/Users/AFast/Documents/python/output_cases.csv")
#court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/250104_output_cases.csv")
court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_cases.csv")
case_actions <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_case_actions.csv")
case_actions %>% count (event_description, sort=T)
court_records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_cases.csv"))
case_actions <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_case_actions.csv"))
joined <- full_join (court_records, case_actions, by=c("case_number"="case_num"))
indict <- joined %>%
filter (str_detect(event_description, "INDICT")) %>%
mutate (event_date = mdy(event_date)) %>%
arrange (event_date) %>%
select (event_date, everything())
#  summarize (indict_dates = range(event_date))
arrest <- joined %>%
filter (str_detect(event_description, "ARREST"))  %>%
mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
range(court_records$case_number)
#How many cases have been scraped for this year?
as.numeric(substr(max(court_records$case_number), 7, 12))-as.numeric(substr(min(court_records$case_number), 7, 12))
#How many cases, including those with multiples?
n_distinct(court_records$case_number)
court_records %>% count (disposition, sort=T)
charges <- court_records %>%
count (charge_description, sort=T)
court_records %>% count (def_status, sort=T)
court_records %>%
mutate (case_num = substr(case_number, 7, 12)) %>%
summarize (cases_found = n_distinct(case_num),
i_range = range (case_num))
#find sexual assault/rape cases
rape <- court_records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL"))
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (charge_description, sort = T)
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (disposition, sort = T)
#pull these dockets
rape2 <- rape %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha)
#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
#logs <- read_lines("/Users/AFast/Documents/python/cleveland_court_records/output.log")
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs)
logs_df2 <- logs_df %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note"),
extra = "drop",
fill = "right") %>%
filter (row_number()>16)
#orig
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
#keeping mult results
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration))
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
case_list <- logs_df3 %>%
distinct (case_num)
# Generate the full sequence from min to max
full_sequence <- seq(min(logs_df3$case_num, na.rm = TRUE), max(logs_df3$case_num, na.rm = TRUE))
# Find missing numbers in log
missing_numbers <- setdiff(full_sequence, logs_df3$case_num)
# Find missing numbers in court records
full_seq <- seq(min(substr(court_records$case_number, 7,12), na.rm = TRUE), max(substr(court_records$case_number, 7,12), na.rm = TRUE))
no_case_records <- setdiff(full_seq, substr(court_records$case_number, 7,12)) %>%
as_tibble()
#Court records saved fine.
check <- no_case_records %>%
count (substr(value, 1,4))
full_seq2 <- seq(min(substr(case_actions$case_num, 7,12), na.rm = TRUE), max(substr(case_actions$case_num, 7,12), na.rm = TRUE))
no_action_records <- setdiff(full_seq2, substr(case_actions$case_num, 7,12)) %>%
as_tibble()
#Case actions for cases 620800-624500 are missing because of typo in code.
check2 <- no_action_records %>%
count (substr(value, 1,4))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE"))
# Add the start and end indices to the breaks for completeness
#breaks <- c(0, breaks, nrow(logs_df3) + 1)
#manual break for 1/10 redo
which (as.character(logs_df3$timestamp)=="2025-01-10 01:55:08")
which (as.character(logs_df3$timestamp)=="2025-01-10 02:26:35")
which (as.character(logs_df3$timestamp)=="2025-01-10 05:37:02")
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, nrow(logs_df3) + 1)
breaks <- sort(breaks)
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
#Count calls on each IP address
logs_df3a <- logs_df2 %>%
filter (str_detect (case_num, "^\\d|Success")|str_detect(str_squish(case_num), "^Process"))
#where did I reset VPN?
breaks2 <- which(logs_df3a$note=="ERROR TERMS OF SERVICE")
#starting after break 6 (7749)
logs_df3b <- logs_df3a %>%
filter (row_number() > breaks2[6]) %>%
filter (str_detect (case_num, "^Success")) %>%
mutate (proxy = str_remove_all(str_remove_all(case_num, "[^0-9.:]"), "^:+")) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
arrange (proxy, timestamp) %>%
group_by (proxy) %>%
mutate (duration = timestamp - lag(timestamp))
ip_count <- logs_df3b %>%
group_by (proxy) %>%
summarize (count = n(),
med = median (duration, na.rm=T),
mean = mean (duration, na.rm=T),
recent = max(timestamp)) %>%
arrange (desc(count))
#keeping mult results
ip_count <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration))
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
ip_count <- logs_df %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip"),
#extra = "drop",
fill = "right") %>%
#keep since i started tracking Ip address
filter (ymd_hms(substr(timestamp,1,19)) > ymd_hms("2025-01-12 11:14:25")) %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "^Name: ASP.NET_SessionId")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num"))
#view(ip_count)
# Function to calculate the total request pulls, case num range, durations, and timestamp min/max
calculate_summary <- function(set) {
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow())  # Add 1 for each additional case result
case_range <- range(set$case_num, na.rm = TRUE)
case_diff <- diff(case_range)  # Calculate the difference between max and min
# Calculate the min and max of timestamp
timestamp_min <- min(set$timestamp, na.rm = TRUE)
timestamp_max <- max(set$timestamp, na.rm = TRUE)
durations <- list(
median_duration = median(set$duration, na.rm = TRUE),
mean_duration = round(mean(set$duration, na.rm = TRUE), 2)
)
tibble(
pulls = pulls,
min_case = case_range[1],
max_case = case_range[2],
diff = case_diff,
median = durations$median_duration,
mean = durations$mean_duration,
start_timestamp = timestamp_min,
end_timestamp = timestamp_max
)
}
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds)
# View the summary
#view(summary)
#this will be slightly higher than count in total_pulls because it includes re-starting sessionIds.
sum(ip_count$count)
#session resets
sessions <- logs_df2 %>%
filter (str_detect(case_num, "Name: ASP.NET_SessionId, Value: ")) %>%
mutate (session_id = str_remove_all(case_num, "Name: ASP.NET_SessionId, Value: "))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp)) %>%
select (-case_num)
#after switch to Proxy()
logs_df3 %>%
filter (row_number()>8955) %>%
summarize (med = median (duration, na.rm=T),
mean = mean (duration, na.rm=T)) %>%
mutate (cases_per_hr = 3600/as.numeric(med))
#recache times
recaching <- logs_df2 %>%
filter (str_detect(case_num, "^Cache expired"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
ip_crosswalk <- tibble(ip="134.238.200.155", gateway="US West", date="1/12/25", set="14") %>%
add_row(ip="208.127.87.65", gateway="US Southwest", date="1/12/25", set="15") %>%
add_row(ip="134.238.14.19", gateway="India West", date="1/12/25", set="16") %>%
add_row(ip="134.238.39.64", gateway="Australia Southeast", date="1/12/25", set="17") %>%
add_row(ip="208.127.73.48", gateway="US East", date="1/13/25", set="18") %>%
add_row(ip="208.127.67.72", gateway="US East", date="1/13/25", set="19") %>%
add_row(ip="165.1.204.206", gateway="US Northeast", date="1/13/25", set="20") %>% #blocked
add_row(ip="208.127.73.48", gateway="US East", date="1/13/25", set="21") %>%
add_row(ip="165.1.204.207", gateway="US Northeast", date="1/13/25", set="22") %>%
add_row(ip="165.1.204.207", gateway="US Northeast", date="1/13/25", set="23") %>%
add_row(ip="208.127.67.72", gateway="US East", date="1/13/25", set="24")%>%
add_row(ip="130.41.15.98", gateway="??", date="1/14/25", set="25")
ip_extra <- logs_df %>%
filter (row_number() > 47744) %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip", "gateway"),
extra = "drop",
fill = "right") %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: "))%>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (case_num = as.numeric(case_num),
date = ymd(substr(timestamp, 1, 10))) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down") %>%
group_by (ip, gateway, date) %>%
summarize (min_case = min(case_num),
max_case = max(case_num),
min_time = min(timestamp),
max_time = max(timestamp)) %>%
mutate (ip = str_remove_all (ip, "IP:"),
gateway = str_remove_all (gateway, "GW:")) %>%
#filter (!is.na(ip)) %>%
select (ip, gateway, min_case, date)
summary2 <- summary %>%
rownames_to_column("set") %>%
full_join (ip_crosswalk, by="set") %>%
mutate (set = as.numeric(set)) %>%
full_join (ip_extra, by="min_case") %>%
mutate (date.x = ymd(date.x)) %>%
mutate (date = coalesce (date.x, date.y),
ip = coalesce (ip.x, ip.y),
gateway = coalesce (gateway.x, gateway.y)) %>%
arrange (desc(set)) %>%
select (-c(gateway.x, gateway.y,ip.x, ip.y,date.x, date.y))
#att 9:40 1/13/25: 166.199.98.115
#view(summary2)
view(summary2)
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
