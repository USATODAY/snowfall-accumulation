r_poly_sf2 <- r_poly_sf %>%
group_by (color_factor) %>%
summarize (geometry = st_union (geometry)) %>%
#drop_na() %>% # the 48h and 72h file had some NA outline showing as black, removing those
filter (color_factor != 0) %>%
ungroup() %>%
select (color_factor)
}
# assign color categories based on the breaks we've defined for USAT
r_poly_sf$color_factor <- cut(r_poly_sf[[column_name]],
breaks = breaks2,
labels = labels2,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
# now assign colors to the color_factor
r_poly_sf$colors <- colors[as.numeric(r_poly_sf$color_factor)]
# spatial join all the 0.1 values between breaks together. This will reduce from hundreds of polygons to <=18 categories.
r_poly_sf2 <- r_poly_sf %>%
group_by (color_factor, colors) %>%
summarize (geometry = st_union (geometry)) %>%
#drop_na() %>% # the 48h and 72h file had some NA outline showing as black, removing those
filter (color_factor != "0 in") %>%
ungroup() %>%
select (color_factor)
check <- r_poly_sf2 %>% st_drop_geometry()
# spatial join all the 0.1 values between breaks together. This will reduce from hundreds of polygons to <=18 categories.
#r_poly_sf2 <- r_poly_sf %>%
#mutate (accumulation = r_poly_sf[[column_name]]/10) %>%
#select (accumulation, color_factor) %>%
#filter (color_factor != "0 in")
#Validate geometry and then simplify slightly to reduce file size
r_poly_sf3 <- r_poly_sf2 %>%
st_make_valid() %>%
st_simplify(dTolerance = 10)
return(r_poly_sf3)
}
snow_list[[4]] <- raster2vector_season()
# adding "season" now
timeframes <- c(timeframes, "season_")
# get some additional info for the chatter
ocr_text <- function(timeframe){
#timeframe = "24h_"
# make a url dynamically
if (timeframe == "season_") {
# first get current year
current_year <- as.numeric(format(Sys.Date(), "%Y"))
# determine the start of the season (oct 1 of the current or previous year)
season_start_year <- if (format(Sys.Date(), "%m") < "10") {
current_year - 1  # ff it's before oct, use the previous year
} else {
current_year  # otherwise, use the current year
}
season_start <- paste0(season_start_year, "093012")  # october 1, 12 UTC
# define the end of the season as the current date
season_end <- paste0(format(Sys.Date(), "%Y%m%d"), "12") # hour fixed, no "00" files
#season_end <- "2025013012"
# connect all this for seasonal URL
path_to_image <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date(), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_", season_end, ".png")
} else {
path_to_image <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/", format(Sys.Date(), "%Y%m"), "/sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour, ".png")
}
#print (paste("Trying", path_to_image))
# extract text
# make it all caps, since sometimes it reads "issued" and sometimes "Issued" - this helps subsetting later
#text <- tesseract::ocr(path_to_image) %>% toupper()
text <- try(tesseract::ocr(path_to_image) %>% toupper())
# Check if path_to_image for season hits 404 error because it hasn't been created yet.
if (timeframe == "season_"){
if (inherits(text, "try-error")) {
#Create path to one day behind the version just tried (1/28/25 instead of 1/29/25, etc.)
new_path_to_image <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date()-days(1), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_",
format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".png")
print(paste(substr(path_to_image, nchar(path_to_image) - 39, nchar(path_to_image)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_image, nchar(new_path_to_image) - 39, nchar(new_path_to_image))))
text <- tesseract::ocr(new_path_to_image) %>% toupper()
print(paste("Pulled", substr(new_path_to_image, nchar(new_path_to_image) - 39, nchar(new_path_to_image))))
} else {
print(paste("Pulled", substr(path_to_image, nchar(path_to_image) - 39, nchar(path_to_image))))
}
}
# Check if path_to_image for other three timeframes hits 404 error because it hasn't been created yet.
if (timeframe != "season_"){
if (inherits(text, "try-error")) {
new_path_to_image <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/",
if_else (hour==12, format(Sys.Date(), "%Y%m"), format(Sys.Date()-days(1), "%Y%m")),
"/sfav2_CONUS_", timeframe,
if_else (hour==12, format(Sys.Date(), "%Y%m%d"), format(Sys.Date()-days(1), "%Y%m%d")),
if_else (hour==12, "00", "12"), ".png")
print(paste(substr(path_to_image, nchar(path_to_image) - 29, nchar(path_to_image)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_image, nchar(new_path_to_image) - 29, nchar(new_path_to_image))))
text <- tesseract::ocr(new_path_to_image) %>% toupper()
print(paste("Pulled", substr(new_path_to_image, nchar(new_path_to_image) - 29, nchar(new_path_to_image))))
} else {
print(paste("Pulled", substr(path_to_image, nchar(path_to_image) - 29, nchar(path_to_image))))
}
}
# get the hour data was updated
#data_updated <- ymd_hms(str_extract(text, "(?<=ISSUED\\s).*?UTC"))
data_updated <-str_extract(text, "(?<=ISSUED\\s).*?UTC")
return(data_updated)
}
# iterate the function for 24hr/48hr/72hr/season accumulations to get last updated data from .png
ocr_list <- lapply(timeframes, ocr_text)
#Save last updated times to JSON
ocr_times <- ocr_list %>% unlist()
last_updated <- tibble(timeframe = str_remove_all(timeframes, "_"), last_updated=ocr_times)
#Convert list to dates for saving properly
ocr_list <- lapply(ocr_list, ymd_hms)
#Save as TopoJSON, overwriting "latest" file.
save_files1 <- function(x){
#geojsonio::topojson_write(snow_list,
geojsonio::topojson_write(snow_list[[x]],
#file = paste0("outputs/", str_remove(timeframes[x], "_"), "/", timeframes[x], "inches_snow_accumulation_latest.json"),
file = paste0("outputs/test/", timeframes[x], "inches_snow_accumulation_latest.json"),
object_name = "snowfall",
overwrite = TRUE)
}
lapply (1:length(timeframes), save_files1)
setwd("~/Documents/GitHub/snowfall-accumulation")
#Save as TopoJSON, overwriting "latest" file.
save_files1 <- function(x){
#geojsonio::topojson_write(snow_list,
geojsonio::topojson_write(snow_list[[x]],
#file = paste0("outputs/", str_remove(timeframes[x], "_"), "/", timeframes[x], "inches_snow_accumulation_latest.json"),
file = paste0("outputs/test/", timeframes[x], "inches_snow_accumulation_latest.json"),
object_name = "snowfall",
overwrite = TRUE)
}
lapply (1:length(timeframes), save_files1)
#RESTART CODE
#r_poly_sf <- st_as_sf(r_poly)
#Try using breaks at every inch up to a foot, then quarter foot up to 5 feet, then every foot up to max value
#breaks3 <- c(-0.01, 0, seq(1,12), seq(15, 120, by=3), seq(132, max(r_poly_sf[[column_name]])+12, by=12))
breaks3 <- c(-0.01, 0, seq(1, max(r_poly_sf[[column_name]])+1))
current_year <- as.numeric(format(Sys.Date(), "%Y"))
# determine the start of the season (oct 1 of the current or previous year)
season_start_year <- if (format(Sys.Date(), "%m") < "10") {
current_year - 1  # ff it's before oct, use the previous year
} else {
current_year  # otherwise, use the current year
}
season_start <- paste0(season_start_year, "093012")  # october 1, 12 UTC
# define the end of the season as the current date
season_end <- paste0(format(Sys.Date(), "%Y%m%d"), "12") # hour fixed, no "00" files
# connect all this for seasonal URL
path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date(), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_", season_end, ".tif")
# load the raster
#r <- rast(path_to_raster)
r <- try(rast(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one day behind the version just tried (1/28/25 instead of 1/29/25, etc.)
new_path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date()-days(1), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_",
format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
r <- rast(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster))))
}
# round raster values to 1 decimal place to capture those between 0-0.1, or else they'll all be rounded to 0 since as.polygons() outputs the nearest integer
r_rounded <- round(r, 1)*10
# convert to polygons with rounded raster values
r_poly <- as.polygons(r_rounded)
# convert raster polygons to sf
r_poly_sf <- st_as_sf(r_poly)
accumulation2 = TRUE
#RESTART CODE
#r_poly_sf <- st_as_sf(r_poly)
#Try using breaks at every inch up to a foot, then quarter foot up to 5 feet, then every foot up to max value
#breaks3 <- c(-0.01, 0, seq(1,12), seq(15, 120, by=3), seq(132, max(r_poly_sf[[column_name]])+12, by=12))
breaks3 <- c(-0.01, 0, seq(1, max(r_poly_sf[[column_name]])+1))
# form what the column name should be
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", season_end)
#RESTART CODE
#r_poly_sf <- st_as_sf(r_poly)
#Try using breaks at every inch up to a foot, then quarter foot up to 5 feet, then every foot up to max value
#breaks3 <- c(-0.01, 0, seq(1,12), seq(15, 120, by=3), seq(132, max(r_poly_sf[[column_name]])+12, by=12))
breaks3 <- c(-0.01, 0, seq(1, max(r_poly_sf[[column_name]])+1))
breaks3 = breaks3*10
labels3 <- paste(breaks3[2:length(breaks3)]/10, "in")
labels3 <- breaks3[2:length(breaks3)]/10
# used https://imagecolorpicker.com/ with the png file to set these 19 colors based on NOAA scale.
#colors2 <- c("#FFFFFF", "#BDD7E7", "#6AAFD6", "#2E83BF", "#024F9B",
#             "#022195", "#FEFE96", "#FEC501", "#FD8900", "#DB0C00",
#             "#9E0101", "#690100")
#labels2 <- c("0 in", "0.1 in", "1 in", "2 in", "3 in", "6 in",
#             "1 ft", "2 ft", "3 ft", "10 ft", "20 ft",
#             "> 30 ft")
# assign color categories based on the breaks we've defined for USAT
r_poly_sf$color_factor <- cut(r_poly_sf[[column_name]],
breaks = breaks3,
labels = labels3,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
check <- r_poly_sf %>% st_drop_geometry()
View(check)
r_poly_sf2 <- r_poly_sf %>%
group_by (color_factor) %>%
summarize (geometry = st_union (geometry)) %>%
#drop_na() %>% # the 48h and 72h file had some NA outline showing as black, removing those
filter (color_factor != 0) %>%
ungroup() %>%
select (color_factor)
#Validate geometry and then simplify slightly to reduce file size
r_poly_sf3 <- r_poly_sf2 %>%
st_make_valid() %>%
st_simplify(dTolerance = 10)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
names(r_poly_sf3)
r_poly_sf3 <- r_poly_sf3 %>%
rename (accumulation = color_factor)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full.json.gz",
overwrite = TRUE)
library(R.utils)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full.json.gz",
overwrite = TRUE)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full.json.gz",
overwrite = TRUE)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full.json",
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full.json.gz",
overwrite = TRUE)
old_name <- "outputs/test/season_inches_snow_accumulation_latest.json.gz"
new_name <- "outputs/test/season_inches_snow_accumulation_latest.json"
# Rename the file
file.rename(old_name, new_name)
old_name <- "outputs/test/season_inches_snow_accumulation_latest_full.json.gz"
new_name <- "outputs/test/season_inches_snow_accumulation_latest_full.json"
# Rename the file
file.rename(old_name, new_name)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full.json.gz",
overwrite = TRUE)
# Path to the gzipped TopoJSON file
gzipped_file <- "outputs/test/season_inches_snow_accumulation_latest_full.json.gz"
# Open the gzipped file and read it
topojson_data <- geojsonio::geojson_read(gzfile(gzipped_file), what = "sp")
# Path to the gzipped TopoJSON file
gzipped_file <- "outputs/test/season_inches_snow_accumulation_latest_full.json.gz"
# Use gzfile() to open and read the content
gz_con <- gzfile(gzipped_file, "rt")  # Open in text mode for reading
# Read the content of the gzipped TopoJSON
topojson_data <- geojsonio::geojson_read(gz_con, what = "sp")
# Path to the gzipped TopoJSON file
gzipped_file <- "outputs/test/season_inches_snow_accumulation_latest_full.json.gz"
# Create a temporary file to store the decompressed content
temp_file <- tempfile(fileext = ".json")
# Decompress the gzipped file into the temporary file
gunzip(gzipped_file, temp_file)
# Now read the decompressed TopoJSON file
topojson_data <- geojsonio::geojson_read(temp_file, what = "sp")
temp_file
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full.json.gz",
overwrite = TRUE)
str(r_poly_sf3)
r_poly_sf4 <- r_poly_sf3 %>%
mutate (accumulation = as.numeric (accumulation))
str(r_poly_sf4)
str(r_poly_sf4)
geojsonio::topojson_write(r_poly_sf4,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
geojson_data <- geojson_json(r_poly_sf3)
geojson_data <- geojsonio::geojson_json(r_poly_sf3)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e4,
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e4,
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e3,
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e2,
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
precision = 1,
quantization = 1e2,
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
overwrite = TRUE)
topojson_quantized <- ms_quantize(geojson_data, quantization = 1e4)  # Adjust quantization level
library(rmapshaper)
# Simplify the polygons
sf_simplified <- ms_simplify(geojson_data, keep = 0.05)  # Adjust keep value (0.01-0.1 works well)
geojsonio::topojson_write(sf_simplified,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
overwrite = TRUE)
# Simplify the polygons
sf_simplified <- ms_simplify(geojson_data, keep = 0.1)  # Adjust keep value (0.01-0.1 works well)
geojsonio::topojson_write(sf_simplified,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
overwrite = TRUE)
# Simplify the polygons
sf_simplified <- ms_simplify(geojson_data, keep = 0.01)  # Adjust keep value (0.01-0.1 works well)
# Simplify the polygons
sf_simplified <- ms_simplify(geojson_data, keep = 0.001)  # Adjust keep value (0.01-0.1 works well)
geojsonio::topojson_write(sf_simplified,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
overwrite = TRUE)
accumulation = TRUE
if (accumulation==TRUE){ #Breaks at every inch up to a foot, then quarter foot up to 5 feet, then every foot up to max value
#RESTART CODE
r_poly_sf <- st_as_sf(r_poly)
breaks3 <- c(-0.01, 0, seq(1,12), seq(15, 60, by=3), seq(72, max(r_poly_sf[[column_name]])+12, by=12))
breaks3 = breaks3*10
labels3 <- paste(breaks3[2:length(breaks3)]/10, "in")
labels3 <- breaks3[2:length(breaks3)]/10
# used https://imagecolorpicker.com/ with the png file to set these 19 colors based on NOAA scale.
#colors2 <- c("#FFFFFF", "#BDD7E7", "#6AAFD6", "#2E83BF", "#024F9B",
#             "#022195", "#FEFE96", "#FEC501", "#FD8900", "#DB0C00",
#             "#9E0101", "#690100")
#labels2 <- c("0 in", "0.1 in", "1 in", "2 in", "3 in", "6 in",
#             "1 ft", "2 ft", "3 ft", "10 ft", "20 ft",
#             "> 30 ft")
# assign color categories based on the breaks we've defined for USAT
r_poly_sf$color_factor <- cut(r_poly_sf[[column_name]],
breaks = breaks3,
labels = labels3,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
check <- r_poly_sf %>% st_drop_geometry()
r_poly_sf2 <- r_poly_sf %>%
group_by (color_factor) %>%
summarize (geometry = st_union (geometry)) %>%
#drop_na() %>% # the 48h and 72h file had some NA outline showing as black, removing those
filter (color_factor != 0) %>%
ungroup() %>%
select (color_factor)
}
#Validate geometry and then simplify slightly to reduce file size
r_poly_sf3 <- r_poly_sf2 %>%
st_make_valid() %>%
st_simplify(dTolerance = 10)
geojson_data <- geojsonio::geojson_json(r_poly_sf3)
# Simplify the polygons
sf_simplified <- ms_simplify(geojson_data, keep = 0.05)  # Adjust keep value (0.01-0.1 works well)
geojsonio::topojson_write(sf_simplified,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e3,
overwrite = TRUE)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e3,
overwrite = TRUE)
accumulation2 = TRUE #break at every inch
if (accumulation2==TRUE){
#RESTART CODE
r_poly_sf <- st_as_sf(r_poly)
breaks3 <- c(-0.01, 0, seq(1, max(r_poly_sf[[column_name]])+1))
breaks3 = breaks3*10
labels3 <- paste(breaks3[2:length(breaks3)]/10, "in")
labels3 <- breaks3[2:length(breaks3)]/10
# used https://imagecolorpicker.com/ with the png file to set these 19 colors based on NOAA scale.
#colors2 <- c("#FFFFFF", "#BDD7E7", "#6AAFD6", "#2E83BF", "#024F9B",
#             "#022195", "#FEFE96", "#FEC501", "#FD8900", "#DB0C00",
#             "#9E0101", "#690100")
#labels2 <- c("0 in", "0.1 in", "1 in", "2 in", "3 in", "6 in",
#             "1 ft", "2 ft", "3 ft", "10 ft", "20 ft",
#             "> 30 ft")
# assign color categories based on the breaks we've defined for USAT
r_poly_sf$color_factor <- cut(r_poly_sf[[column_name]],
breaks = breaks3,
labels = labels3,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
#check <- r_poly_sf %>% st_drop_geometry()
r_poly_sf2 <- r_poly_sf %>%
group_by (color_factor) %>%
summarize (geometry = st_union (geometry)) %>%
filter (color_factor != 0) %>%
ungroup() %>%
select (color_factor) %>%
rename (accumulation = color_factor)
}
#Validate geometry and then simplify slightly to reduce file size
r_poly_sf3 <- r_poly_sf2 %>%
st_make_valid() %>%
st_simplify(dTolerance = 10)
str(r_poly_sf3)
r_poly_sf4 <- r_poly_sf3 %>%
mutate (accumulation = as.numeric (accumulation))
geojson_data <- geojsonio::geojson_json(r_poly_sf4)
geojsonio::topojson_write(r_poly_sf4,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full.json"),
object_name = "snowfall",
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_full_geo.json"),
object_name = "snowfall",
overwrite = TRUE)
geojsonio::topojson_write(r_poly_sf3,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk1.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e3,
overwrite = TRUE)
geojsonio::topojson_write(geojson_data,
file = paste0("outputs/test/season_inches_snow_accumulation_latest_shrunk2.json"),
object_name = "snowfall",
precision = 0,
quantization = 1e3,
overwrite = TRUE)
gzip("outputs/test/season_inches_snow_accumulation_latest_full2.json",
destname = "outputs/test/season_inches_snow_accumulation_latest_full2.json.gz",
overwrite = TRUE)
json <- jsonlite::fromJSON("/Users/AFast/Documents/cleveland_smith_output.json") %>%
clean_names() %>%
mutate (filing_date = mdy(filing_date),
case_num = substr(case_number, 7,12),
year = paste0(if_else(substr(case_number, 4,5) < 26, "20", "19"), substr(case_number, 4,5)))
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
json <- jsonlite::fromJSON("/Users/AFast/Documents/cleveland_smith_output.json") %>%
clean_names() %>%
mutate (filing_date = mdy(filing_date),
case_num = substr(case_number, 7,12),
year = paste0(if_else(substr(case_number, 4,5) < 26, "20", "19"), substr(case_number, 4,5)))
json2 <- json %>%
group_by (year) %>%
summarize (min = min(case_num),
min_date = filing_date[which.min(case_num)],
max = max(case_num),
max_date = filing_date[which.max(case_num)]) %>%
arrange (desc(year))
View(json2)
