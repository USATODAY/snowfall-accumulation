#              "> 30 ft")
#
# form what the column name should be
#column_name <- paste0("sfav2_CONUS_", season_start, "_to_", season_end)
# assign color categories based on the breaks we've defined for USAT
# r_poly_sf$color_factor <- cut(r_poly_sf[[column_name]],
#                               breaks = breaks2,
#                               labels = labels2,
#                               include.lowest = FALSE,
#                               right = TRUE)  # ensure right endpoint is included
#
# now assign colors to the color_factor
# r_poly_sf$colors <- colors[as.numeric(r_poly_sf$color_factor)]
#
# # spatial join all the 0.1 values between breaks together. This will reduce from hundreds of polygons to <=18 categories.
# r_poly_sf2 <- r_poly_sf %>%
#   group_by (color_factor, colors) %>%
#   summarize (geometry = st_union (geometry)) %>%
#   #drop_na() %>% # the 48h and 72h file had some NA outline showing as black, removing those
#   filter (color_factor != "0 in") %>%
#   ungroup() %>%
#   select (color_factor)
# spatial join all the 0.1 values between breaks together. This will reduce from hundreds of polygons to <=18 categories.
#r_poly_sf2 <- r_poly_sf %>%
#mutate (accumulation = r_poly_sf[[column_name]]/10) %>%
#select (accumulation, color_factor) %>%
#filter (color_factor != "0 in")
# #Validate geometry and then simplify slightly to reduce file size
# r_poly_sf3 <- r_poly_sf2 %>%
#   st_make_valid() %>%
#   st_simplify(dTolerance = 10)
if (version=="stars"){
r <- try(read_stars(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one day behind the version just tried (1/28/25 instead of 1/29/25, etc.)
new_path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date()-days(1), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_",
format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
r <- read_stars(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", season_end, ".tif")
}
r_poly <- st_as_sf(r, merge = TRUE)  # Merge adjacent areas of same value
}
# convert raster polygons to sf
r_poly2 <- st_as_sf(r_poly) %>%
filter (get(column_name) > 0) %>% #remove NAs and 0
rename (accumulation = column_name)
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
#accumulation< 0 ~ -1,
#accumulation == 0 ~ 0,
between(accumulation, 0, 1) ~ 1, #round 0.1-0.4 up to 1
TRUE ~ round(accumulation)))
#check <- r_poly2 %>% st_drop_geometry()
r_poly4 <- r_poly3 %>%
group_by (accumulation) %>%
summarize (geometry=st_union(geometry))
return(r_poly4)
}
system.time(
snow_list[[4]] <- raster2vector_season()
)
202/60
# adding "season" now
timeframes <- c(timeframes, "season_")
#Save as TopoJSON, overwriting "latest" file.
save_files1 <- function(x){
#geojsonio::topojson_write(snow_list,
geojsonio::topojson_write(snow_list[[x]],
#file = paste0("outputs/", str_remove(timeframes[x], "_"), "/", timeframes[x], "inches_snow_accumulation_latest.json"),
file = paste0("outputs/test/", timeframes[x], "inches_snow_accumulation_latest-stars.json"),
#file = paste0("outputs/test/", timeframes[x], "inches_snow_accumulation_latest-rast2.json"),
object_name = "snowfall",
overwrite = TRUE)
}
lapply (4, save_files1)
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
# Function to calculate the total request pulls, case num range, durations, and timestamp min/max
calculate_summary <- function(set) {
#set=sets[1][[1]]
session_resets <- set %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
filter (between(timestamp, min(set$timestamp, na.rm=T), max(set$timestamp, na.rm=T))) %>%
filter (str_detect (case_num, "ASP.NET_SessionId")) %>%
nrow() + 1 #+1 because initial setup is before first timestamp -- 3 pulls for each session reset
if (min(set$timestamp) > ymd("2025-01-16")){
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow() * 2)  # Add 1 for each additional case result + 1 request to go back to search list
(set %>% filter(str_detect(note, "NO CASES")) %>% nrow() * 1)  # Add 1 for GET request back from No cases error page.
} else {
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow()) # Add 1 for each additional case result
}
case_range <- range(set$case_num, na.rm = TRUE)
case_diff <- diff(case_range)  # Calculate the difference between max and min
# Calculate the min and max of timestamp
timestamp_min <- min(set$timestamp, na.rm = TRUE)
timestamp_max <- max(set$timestamp, na.rm = TRUE)
durations <- list(
median_duration = median(set$duration, na.rm = TRUE),
mean_duration = round(mean(set$duration, na.rm = TRUE), 2)
)
#this_ip <- toString (unique(set$ip))
#this_gateway <- toString(unique(set$gateway))
#sometimes it pulls in a few wrong IP addresses or gateway, if screengrab didn't work. This chooses the one with the most.
this_ip <- set %>% count (ip, sort=T) %>% head (1) %>% pull (ip)
this_gateway <- set %>% count (gateway, sort=T) %>% head (1) %>% pull (gateway)
this_comp <- set %>% count (comp, sort=T) %>% head (1) %>% pull (comp)
tibble(
pulls = pulls,
sessions = session_resets,
total = pulls + (sessions*3),
min_case = case_range[1],
max_case = case_range[2],
diff = case_diff,
median = durations$median_duration,
mean = durations$mean_duration,
start_timestamp = timestamp_min,
end_timestamp = timestamp_max,
ip = this_ip,
gateway = this_gateway,
comp = this_comp
)
}
year = "2001"
#court_records <- read_csv ("/Users/AFast/Documents/python/output_cases.csv")
#court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/250104_output_cases.csv")
court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_cases.csv")
case_actions <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_case_actions.csv")
case_actions %>% count (event_description, sort=T)
court_records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_cases.csv"))
case_actions <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_case_actions.csv"))
joined <- full_join (court_records, case_actions, by=c("case_number"="case_num"))
if (year >= 2004){
indict <- joined %>%
filter (str_detect(event_description, "INDICT")) %>%
mutate (event_date = mdy(event_date)) %>%
arrange (event_date) %>%
select (event_date, everything())
#  summarize (indict_dates = range(event_date))
arrest <- joined %>%
filter (str_detect(event_description, "ARREST"))  %>%
mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
} else {
indict <- joined %>%
filter (str_detect(event_description, "BINDOVER")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ORIGINAL"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date),
count = n())
}
range(court_records$case_number)
#How many cases have been scraped for this year?
as.numeric(substr(max(court_records$case_number), 7, 12))-as.numeric(substr(min(court_records$case_number), 7, 12))
#How many cases, including those with multiples?
n_distinct(court_records$case_number)
court_records %>% count (disposition, sort=T)
charges <- court_records %>%
count (charge_description, sort=T)
court_records %>% count (def_status, sort=T)
court_records %>%
mutate (case_num = substr(case_number, 7, 12)) %>%
summarize (cases_found = n_distinct(case_num),
i_range = range (case_num))
#find sexual assault/rape cases
rape <- court_records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL"))
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (charge_description, sort = T)
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (disposition, sort = T)
#pull these dockets
rape2 <- rape %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha)
#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
#Combine work laptop and personal laptop logs
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work") #%>%
#add line to split between work and personal if no number end of one.
logs_df <- logs_df %>%
add_row (line = paste(ymd_hms(substr(str_split(tail(logs_df$line, 1), " - ")[[1]][1], 1, 19)) + seconds(1), "- Switching VPN combine work and pers"), comp="work")
logs2 <- read_lines("/Users/AFast/Documents/python/output_private.log")
logs_df2 <- tibble(line = logs2) %>%
mutate (comp = "pers")
logs_df <- rbind (logs_df, logs_df2)
#logs_df <- tibble(line = logs3)
# END NEW CODE
#logs <- read_lines("/Users/AFast/Documents/python/cleveland_court_records/output.log")
#READ JUST THIS TO THIS COMPUTER'S LOG
read = FALSE
if (read==TRUE){
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work")
}
logs_df2 <- logs_df %>%
#logs_df2 <- logs3 %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip", "gateway"),
extra = "drop",
fill = "right") %>%
filter (row_number()>16)
#keeping mult results
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (note = case_when (
str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop") ~ case_num,
#case_num %in% c("2100 iterations. Switching VPN", "Stopping loop: 20 consecutive 'NO CASES' encountered.") ~ case_num,
TRUE ~ note)) %>% #"2100 iterations. Switching VPN", note)) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration)) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down")
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE")|str_detect(logs_df3$note, "Switching VPN")|str_detect(logs_df3$note, "Stopping loop"))
#Find changes in IP address to add to breaks vector
gw_numbers2 <- str_extract(logs_df3$gateway, "(?<=GW:United States #)\\d+")
# Convert to numeric and identify changes
gw_numbers_numeric <- as.numeric(gw_numbers2)
#changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) %>% as.integer()
changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) - 1
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, 36338, 37285, 37837, 37859, 65442, nrow(logs_df3) + 1, changes)
breaks <- unique(sort(breaks))
#first row after IP change is getting excluded when two breaks follow each other sequentially -- keep first
breaks <- breaks[c(TRUE, diff(breaks) != 1)]
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
# Keep only elements with more than one row: two ERROR TERMS IN SERVICE in a row results in a 1-row set.
sets <- keep(sets, ~ nrow(.) > 1)
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds) %>%
arrange (desc(end_timestamp)) %>%
mutate (date = ymd(substr(end_timestamp, 1, 10)),
ip = str_squish(str_remove_all(ip, "IP:")),
gateway = str_squish(str_remove_all(gateway, "GW:"))) %>%
mutate (gateway = case_when (
ip=="134.238.200.155" ~ "US West",
ip=="208.127.87.65" ~ "US Southwest",
ip=="134.238.14.19" ~ "India West",
ip=="134.238.39.64" ~ "Australia Southeast",
ip=="208.127.73.48" ~ "US East",
ip=="208.127.67.72" ~ "US East",
ip=="165.1.204.206" ~ "US Northeast",
ip=="165.1.204.207" ~ "US Northeast",
ip=="130.41.15.98" ~ "??",
is.na(gateway) ~ "Unknown",
TRUE ~ gateway
))
# View the summary
#view(summary)
#session resets
sessions <- logs_df2 %>%
filter (str_detect(case_num, "Name: ASP.NET_SessionId, Value: ")) %>%
mutate (session_id = str_remove_all(case_num, "Name: ASP.NET_SessionId, Value: "))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp)) %>%
select (-case_num)
#recache times
recaching <- logs_df2 %>%
filter (str_detect(case_num, "^Cache expired"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
summary2 <- summary %>%
filter (date == Sys.Date()) %>%
arrange (comp)
view(summary2)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary4 <- summary3 %>% group_by (date) %>% summarize (total=sum(cases))
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(tesseract)
library(lubridate)
library(jsonlite)
library(tibble)
library(stars)
# Form path to URL: First, check the time and output 12 or 00 -- use 12 after 1 p.m. UTC. Otherwise, use 00.
hour <- if (as.numeric(format(Sys.time(), "%H")) >= 13 && as.numeric(format(Sys.time(), "%H")) < 24) {
"12"
} else {
"00"
}
timeframes <- c("24h_", "48h_", "72h_")
version = "stars"
timeframe <- "24h_"
#timeframe <- "24h_"
# make a url
path_to_raster <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/", format(Sys.Date(), "%Y%m"), "/sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour, ".tif")
if (version=="terra"){
# load the raster
r <- try(rast(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one version behind (if tried 12 UTC for 1/29/25, create path for 00 UTC 1/29/25, etc.)
new_path_to_raster <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/",
if_else (hour==12, format(Sys.Date(), "%Y%m"), format(Sys.Date()-days(1), "%Y%m")),
"/sfav2_CONUS_", timeframe,
if_else (hour==12, format(Sys.Date(), "%Y%m%d"), format(Sys.Date()-days(1), "%Y%m%d")),
if_else (hour==12, "00", "12"), ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 29, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 29, nchar(new_path_to_raster))))
r <- rast(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 29, nchar(new_path_to_raster))))
#Set column name to previous version
column_name <- paste0("sfav2_CONUS_", timeframe, if_else (hour==12, format(Sys.Date(), "%Y%m%d"), format(Sys.Date()-days(1), "%Y%m%d")),
if_else (hour==12, "00", "12"))
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 29, nchar(path_to_raster))))
#Use current version of column name
column_name <- paste0("sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour)
}
print (paste("Using column:", column_name))
# convert to polygons with rounded raster values
r_poly <- as.polygons(r,
round=TRUE,
digits=1, #round to nearest 1/10"
aggregate=TRUE) #false produces separate features for all with same accumulation value.
# convert raster polygons to sf
r_poly_sf2 <- st_as_sf(r_poly) %>%
filter (get(column_name) > 0) %>%
rename (accumulation = column_name)
}
if (version=="stars"){
# load the raster
r <- try(read_stars(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one version behind (if tried 12 UTC for 1/29/25, create path for 00 UTC 1/29/25, etc.)
new_path_to_raster <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/",
if_else (hour==12, format(Sys.Date(), "%Y%m"), format(Sys.Date()-days(1), "%Y%m")),
"/sfav2_CONUS_", timeframe,
if_else (hour==12, format(Sys.Date(), "%Y%m%d"), format(Sys.Date()-days(1), "%Y%m%d")),
if_else (hour==12, "00", "12"), ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 29, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 29, nchar(new_path_to_raster))))
r <- read_stars(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 29, nchar(new_path_to_raster))))
#Set column name to previous version
column_name <- paste0("sfav2_CONUS_", timeframe, if_else (hour==12, format(Sys.Date(), "%Y%m%d"), format(Sys.Date()-days(1), "%Y%m%d")),
if_else (hour==12, "00", "12"), ".tif")
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 29, nchar(path_to_raster))))
#Use current version of column name
column_name <- paste0("sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour, ".tif")
}
print (paste("Using column:", column_name))
# Convert raster to polygons
r_poly2 <- st_as_sf(r, merge = TRUE)  # Merge adjacent areas of same value
# Round to the nearest tenth of an inch.
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
get(column_name) < 0 ~ -1,
get(column_name) == 0 ~ 0,
between(column_name, 0, 0.1) ~ 0.1, #anything below 0.1" rounds up to 0.1"
TRUE ~ round(get(column_name), 1))) #round everything else to nearest 1/10"
#check <- r_poly3 %>% st_drop_geometry()
#check2 <- check %>% count (accumulation)
r_poly_sf2 <- r_poly3 %>%
group_by (accumulation) %>%
summarize (geometry=st_union(geometry)) %>%
filter (accumulation > 0)
#check3 <- r_poly4 %>% st_drop_geometry()
}
names(r_poly2)
column_name
# Round to the nearest tenth of an inch.
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
get(column_name) < 0 ~ -1,
get(column_name) == 0 ~ 0,
between(column_name, 0, 0.1) ~ 0.1, #anything below 0.1" rounds up to 0.1"
TRUE ~ round(get(column_name), 1))) #round everything else to nearest 1/10"
names(r_poly2)
# Round to the nearest tenth of an inch.
r_poly3 <- r_poly2 %>%
rename (column = get(column_name))
column_name %in% names(r_poly2)
# Round to the nearest tenth of an inch.
r_poly3 <- r_poly2 %>%
rename (column = get(column_name))
# Round to the nearest tenth of an inch.
r_poly3 <- r_poly2 %>%
rename (column = get(column_name))
names(r_poly2)
column_name
# Round to the nearest tenth of an inch.
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
get(column_name) < 0 ~ -1,
get(column_name) == 0 ~ 0,
between(get(column_name), 0, 0.1) ~ 0.1, #anything below 0.1" rounds up to 0.1"
TRUE ~ round(get(column_name), 1))) #round everything else to nearest 1/10"
