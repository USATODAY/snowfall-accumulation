redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
view(summary)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2006 cases:", summary$min_case[1]-475604))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
View(json2)
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
year = "2006"
#court_records <- read_csv ("/Users/AFast/Documents/python/output_cases.csv")
#court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/250104_output_cases.csv")
court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_cases.csv")
case_actions <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_case_actions.csv")
case_actions %>% count (event_description, sort=T)
court_records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_cases.csv"))
case_actions <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_case_actions.csv"))
joined <- full_join (court_records, case_actions, by=c("case_number"="case_num"))
indict <- joined %>%
filter (str_detect(event_description, "INDICT")) %>%
mutate (event_date = mdy(event_date)) %>%
arrange (event_date) %>%
select (event_date, everything())
#  summarize (indict_dates = range(event_date))
arrest <- joined %>%
filter (str_detect(event_description, "ARREST"))  %>%
mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
range(court_records$case_number)
#How many cases have been scraped for this year?
as.numeric(substr(max(court_records$case_number), 7, 12))-as.numeric(substr(min(court_records$case_number), 7, 12))
#How many cases, including those with multiples?
n_distinct(court_records$case_number)
court_records %>% count (disposition, sort=T)
charges <- court_records %>%
count (charge_description, sort=T)
court_records %>% count (def_status, sort=T)
court_records %>%
mutate (case_num = substr(case_number, 7, 12)) %>%
summarize (cases_found = n_distinct(case_num),
i_range = range (case_num))
#find sexual assault/rape cases
rape <- court_records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL"))
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (charge_description, sort = T)
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (disposition, sort = T)
#pull these dockets
rape2 <- rape %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha)
#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
#Combine work laptop and personal laptop logs
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work") #%>%
#add line to split between work and personal if no number end of one.
logs_df <- logs_df %>%
add_row (line = paste(ymd_hms(substr(str_split(tail(logs_df$line, 1), " - ")[[1]][1], 1, 19)) + seconds(1), "- Switching VPN combine work and pers"), comp="work")
logs2 <- read_lines("/Users/AFast/Documents/python/output_private.log")
logs_df2 <- tibble(line = logs2) %>%
mutate (comp = "pers")
logs_df <- rbind (logs_df, logs_df2)
#logs_df <- tibble(line = logs3)
# END NEW CODE
#logs <- read_lines("/Users/AFast/Documents/python/cleveland_court_records/output.log")
#READ JUST THIS TO THIS COMPUTER'S LOG
read = FALSE
if (read==TRUE){
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work")
}
logs_df2 <- logs_df %>%
#logs_df2 <- logs3 %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip", "gateway"),
extra = "drop",
fill = "right") %>%
filter (row_number()>16)
#orig
#logs_df3 <- logs_df2 %>%
#  filter (str_detect (case_num, "^\\d")) %>%
#  mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
#  mutate (duration = timestamp - lag(timestamp))
#keeping mult results
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (note = case_when (
str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop") ~ case_num,
#case_num %in% c("2100 iterations. Switching VPN", "Stopping loop: 20 consecutive 'NO CASES' encountered.") ~ case_num,
TRUE ~ note)) %>% #"2100 iterations. Switching VPN", note)) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration)) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down")
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
case_list <- logs_df3 %>%
distinct (case_num)
# Generate the full sequence from min to max
full_sequence <- seq(min(logs_df3$case_num, na.rm = TRUE), max(logs_df3$case_num, na.rm = TRUE))
# Find missing numbers in log
missing_numbers <- setdiff(full_sequence, logs_df3$case_num)
# Find missing numbers in court records
full_seq <- seq(min(substr(court_records$case_number, 7,12), na.rm = TRUE), max(substr(court_records$case_number, 7,12), na.rm = TRUE))
no_case_records <- setdiff(full_seq, substr(court_records$case_number, 7,12)) %>%
as_tibble()
#Court records saved fine.
check <- no_case_records %>%
count (substr(value, 1,4))
full_seq2 <- seq(min(substr(case_actions$case_num, 7,12), na.rm = TRUE), max(substr(case_actions$case_num, 7,12), na.rm = TRUE))
no_action_records <- setdiff(full_seq2, substr(case_actions$case_num, 7,12)) %>%
as_tibble()
#Case actions for cases 620800-624500 are missing because of typo in code.
check2 <- no_action_records %>%
count (substr(value, 1,4))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE")|str_detect(logs_df3$note, "Switching VPN"))#|str_detect(logs_df3$note, "Stopping loop"))
#manual after 20 no cases
# Add the start and end indices to the breaks for completeness
#breaks <- c(0, breaks, nrow(logs_df3) + 1)
#manual break for 1/10 redo
which (as.character(logs_df3$timestamp)=="2025-01-10 01:55:08")
which (as.character(logs_df3$timestamp)=="2025-01-10 02:26:35")
which (as.character(logs_df3$timestamp)=="2025-01-10 05:37:02")
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE")|str_detect(logs_df3$note, "Switching VPN")|str_detect(logs_df3$note, "Stopping loop"))
#Find changes in IP address to add to breaks vector
gw_numbers2 <- str_extract(logs_df3$gateway, "(?<=GW:United States #)\\d+")
# Convert to numeric and identify changes
gw_numbers_numeric <- as.numeric(gw_numbers2)
#changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) %>% as.integer()
changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) - 1
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, 36338, 37285, 37837, 37859, 65442, nrow(logs_df3) + 1, changes)
#breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 30185, 36335, nrow(logs_df3) + 1)
number <- 36338
check <- logs_df3 %>%
rownames_to_column() %>%
filter (row_number() > (number-10) & row_number() < (number+10))
breaks <- unique(sort(breaks))
#first row after IP change is getting excluded when two breaks follow each other sequentially -- keep first
breaks <- breaks[c(TRUE, diff(breaks) != 1)]
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
# Keep only elements with more than one row: two ERROR TERMS IN SERVICE in a row results in a 1-row set.
sets <- keep(sets, ~ nrow(.) > 1)
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
#Count calls on each IP address
logs_df3a <- logs_df2 %>%
filter (str_detect (case_num, "^\\d|Success")|str_detect(str_squish(case_num), "^Process"))
#where did I reset VPN?
breaks2 <- which(logs_df3a$note=="ERROR TERMS OF SERVICE")
#starting after break 6 (7749)
logs_df3b <- logs_df3a %>%
filter (row_number() > breaks2[6]) %>%
filter (str_detect (case_num, "^Success")) %>%
mutate (proxy = str_remove_all(str_remove_all(case_num, "[^0-9.:]"), "^:+")) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
arrange (proxy, timestamp) %>%
group_by (proxy) %>%
mutate (duration = timestamp - lag(timestamp))
ip_count <- logs_df3b %>%
group_by (proxy) %>%
summarize (count = n(),
med = median (duration, na.rm=T),
mean = mean (duration, na.rm=T),
recent = max(timestamp)) %>%
arrange (desc(count))
#keeping mult results
ip_count <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration))
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
ip_count <- logs_df %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip"),
#extra = "drop",
fill = "right") %>%
#keep since i started tracking Ip address
filter (ymd_hms(substr(timestamp,1,19)) > ymd_hms("2025-01-12 11:14:25")) %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "^Name: ASP.NET_SessionId")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num"))
#view(ip_count)
# Function to calculate the total request pulls, case num range, durations, and timestamp min/max
calculate_summary <- function(set) {
#set=sets[1][[1]]
session_resets <- logs_df2 %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
filter (between(timestamp, min(set$timestamp, na.rm=T), max(set$timestamp, na.rm=T))) %>%
filter (str_detect (case_num, "ASP.NET_SessionId")) %>%
nrow() + 1 #+1 because initial setup is before first timestamp -- 3 pulls for each session reset
if (min(set$timestamp) > ymd("2025-01-16")){
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow() * 2)  # Add 1 for each additional case result + 1 request to go back to search list
(set %>% filter(str_detect(note, "NO CASES")) %>% nrow() * 1)  # Add 1 for GET request back from No cases error page.
} else {
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow()) # Add 1 for each additional case result
}
case_range <- range(set$case_num, na.rm = TRUE)
case_diff <- diff(case_range)  # Calculate the difference between max and min
# Calculate the min and max of timestamp
timestamp_min <- min(set$timestamp, na.rm = TRUE)
timestamp_max <- max(set$timestamp, na.rm = TRUE)
durations <- list(
median_duration = median(set$duration, na.rm = TRUE),
mean_duration = round(mean(set$duration, na.rm = TRUE), 2)
)
#this_ip <- toString (unique(set$ip))
#this_gateway <- toString(unique(set$gateway))
#sometimes it pulls in a few wrong IP addresses or gateway, if screengrab didn't work. This chooses the one with the most.
this_ip <- set %>% count (ip, sort=T) %>% head (1) %>% pull (ip)
this_gateway <- set %>% count (gateway, sort=T) %>% head (1) %>% pull (gateway)
this_comp <- set %>% count (comp, sort=T) %>% head (1) %>% pull (comp)
tibble(
pulls = pulls,
sessions = session_resets,
total = pulls + (sessions*3),
min_case = case_range[1],
max_case = case_range[2],
diff = case_diff,
median = durations$median_duration,
mean = durations$mean_duration,
start_timestamp = timestamp_min,
end_timestamp = timestamp_max,
ip = this_ip,
gateway = this_gateway,
comp = this_comp
)
}
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds) %>%
arrange (desc(end_timestamp)) %>%
mutate (date = ymd(substr(end_timestamp, 1, 10)),
ip = str_squish(str_remove_all(ip, "IP:")),
gateway = str_squish(str_remove_all(gateway, "GW:"))) %>%
mutate (gateway = case_when (
ip=="134.238.200.155" ~ "US West",
ip=="208.127.87.65" ~ "US Southwest",
ip=="134.238.14.19" ~ "India West",
ip=="134.238.39.64" ~ "Australia Southeast",
ip=="208.127.73.48" ~ "US East",
ip=="208.127.67.72" ~ "US East",
ip=="165.1.204.206" ~ "US Northeast",
ip=="165.1.204.207" ~ "US Northeast",
ip=="130.41.15.98" ~ "??",
is.na(gateway) ~ "Unknown",
TRUE ~ gateway
))
# View the summary
#view(summary)
#this will be slightly higher than count in total_pulls because it includes re-starting sessionIds.
sum(ip_count$count)
#session resets
sessions <- logs_df2 %>%
filter (str_detect(case_num, "Name: ASP.NET_SessionId, Value: ")) %>%
mutate (session_id = str_remove_all(case_num, "Name: ASP.NET_SessionId, Value: "))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp)) %>%
select (-case_num)
#after switch to Proxy()
logs_df3 %>%
filter (row_number()>8955) %>%
summarize (med = median (duration, na.rm=T),
mean = mean (duration, na.rm=T)) %>%
mutate (cases_per_hr = 3600/as.numeric(med))
#recache times
recaching <- logs_df2 %>%
filter (str_detect(case_num, "^Cache expired"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
view(summary)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2006 cases:", summary$min_case[1]-475604))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(tesseract)
library(lubridate)
library(tidyr)
library(geosonio)
#Set time zone to pull files using Eastern time so GitHub Actions doesn't use UTC.
#Sys.setenv(TZ="America/New_York")
Sys.setenv(TZ="UTC")
# Form path to URL: First, check the time and output 12 or 00 -- use 12 after 1 p.m. UTC. Otherwise, use 00.
hour <- if (as.numeric(format(Sys.time(), "%H")) >= 13 && as.numeric(format(Sys.time(), "%H")) < 24) {
"12"
} else {
"00"
}
timeframes <- c("24h_", "48h_", "72h_")
# we'll make a function to get and process the latest raster
raster2vector <- function(timeframe){
# make a url
path_to_raster <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/", format(Sys.Date(), "%Y%m"), "/sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour, ".tif")
# load the raster
r <- rast(path_to_raster)
# round raster values to 1 decimal place to capture those between 0-0.1, or else they'll all be rounded to 0 since as.polygons() outputs the nearest integer
r_rounded <- round(r, 1)*10
# convert to polygons with rounded raster values
r_poly <- as.polygons(r_rounded)
# convert raster polygons to sf
r_poly_sf <- st_as_sf(r_poly)
# define the breaks and colors
breaks <- c(-0.01, 0, 0.1, 1, 2,
3, 4, 6, 8, 12,
18, 24, 30, 36, 48,
60, 72, 96, 120, 500)  # added -0.01 and 500 to handle zero + >120 explicitly
# multiply by 10 to match the rounding done above.
breaks = breaks*10
# used https://imagecolorpicker.com/ with the png file to set these 19 colors based on NOAA scale.
colors <- c("#ffffff", "#e4eef4", "#bdd7e7", "#6bafd6", "#2d83be",
"#02509d", "#022195", "#fefe96", "#ffc500", "#ff8800",
"#dc0c00", "#9f0000", "#690000", "#330000", "#cdcdff",
"#a08dd9", "#7d51a6", "#551573", "#290030")
# define the corresponding labels (numeric values)
labels <- c("0", "0.1", "1", "2", "3",
"4", "6", "8", "12", "18",
"24", "30", "36", "48", "60",
"72", "96", "120", ">120")
# form what the column name should be
column_name <- paste0("sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour)
# assign color categories based on the breaks
r_poly_sf$color_factor <- cut(r_poly_sf[[column_name]],
breaks = breaks,
labels = labels,
include.lowest = FALSE,
right = TRUE)  # ensure right endpoint is included
# now assign colors to the color_factor
r_poly_sf$colors <- colors[as.numeric(r_poly_sf$color_factor)]
# spatial join all the 0.1 values between breaks together. This will reduce from hundreds of polygons to <=18 categories.
r_poly_sf2 <- r_poly_sf %>%
group_by (color_factor, colors) %>%
summarize (geometry = st_union (geometry)) %>%
drop_na() %>% # the 48h and 72h file had some NA outline showing as black, removing those
mutate(accumulation = timeframe)
return(r_poly_sf2)
}
# iterate the function for 24hr, 48hr and 72hr accumulations
snow_list <- lapply(timeframes, raster2vector)
# now get some additional info for the chatter
ocr_text <- function(timeframe){
#timeframe="24h_"
# make a url
path_to_image <- paste0("https://www.nohrsc.noaa.gov/snowfall/data/", format(Sys.Date(), "%Y%m"), "/sfav2_CONUS_", timeframe, format(Sys.Date(), "%Y%m%d"), hour, ".png")
# extract text
# make it all caps, since sometimes it reads "issued" and sometimes "Issued" - this helps subsetting later
text <- tesseract::ocr(path_to_image) %>% toupper()
# get the hour data was updated
data_updated <- ymd_hms(str_extract(text, "(?<=ISSUED\\s).*?UTC"))
return(data_updated)
}
# iterate the function for 24hr, 48hr and 72hr accumulations to get last updated data from .png
ocr_list <- lapply(timeframes, ocr_text)
save_files2 <- function(x){
geojsonio::topojson_write(snow_list[[x]],
file = paste0("outputs/", substr(timeframes[x], 1,3), "/", format(Sys.Date(), "%Y%m%d"), hour, "_", format(ocr_list[[x]], "%H%M%S"), "_", timeframes[x], "snow_accumulation.json"),
object_name = "snowfall",
overwrite = TRUE)
}
lapply (1:length(timeframes), save_files2)
setwd("~/Documents/GitHub/snowfall-accumulation")
save_files2 <- function(x){
geojsonio::topojson_write(snow_list[[x]],
file = paste0("outputs/", substr(timeframes[x], 1,3), "/", format(Sys.Date(), "%Y%m%d"), hour, "_", format(ocr_list[[x]], "%H%M%S"), "_", timeframes[x], "snow_accumulation.json"),
object_name = "snowfall",
overwrite = TRUE)
}
lapply (1:length(timeframes), save_files2)
