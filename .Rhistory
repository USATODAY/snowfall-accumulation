#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
#Set time zone to pull files using Eastern time so GitHub Actions doesn't use UTC.
Sys.setenv(TZ="America/New_York")
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
# Set the directory containing the .opus files
input_dir <- "/Users/AFast/Downloads/Curtis court audio"
# Get the list of all .opus files in the directory
opus_files <- list.files(input_dir, pattern = "\\.opus$", full.names = TRUE)
opus_files
opus_files <- opus_files[1:5]
for (opus_file in opus_files) {
# Construct the output file name by replacing .opus with .mp3
mp3_file <- sub("\\.opus$", ".mp3", opus_file)
# Construct the ffmpeg command
cmd <- sprintf('ffmpeg -i "%s" -acodec libmp3lame -q:a 2 "%s"', opus_file, mp3_file)
# Run the command
system(cmd, ignore.stdout = TRUE, ignore.stderr = TRUE)
}
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
json <- jsonlite::fromJSON("/Users/AFast/Documents/cleveland_smith_output.json") %>%
clean_names() %>%
mutate (filing_date = mdy(filing_date),
case_num = substr(case_number, 7,12),
year = paste0(if_else(substr(case_number, 4,5) < 26, "20", "19"), substr(case_number, 4,5)))
json2 <- json %>%
group_by (year) %>%
summarize (min = min(case_num),
min_date = filing_date[which.min(case_num)],
max = max(case_num),
max_date = filing_date[which.max(case_num)]) %>%
arrange (desc(year))
View(json2)
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
json <- jsonlite::fromJSON("/Users/AFast/Documents/cleveland_smith_output.json") %>%
clean_names() %>%
mutate (filing_date = mdy(filing_date),
case_num = substr(case_number, 7,12),
year = paste0(if_else(substr(case_number, 4,5) < 26, "20", "19"), substr(case_number, 4,5)))
json2 <- json %>%
group_by (year) %>%
summarize (min = min(case_num),
min_date = filing_date[which.min(case_num)],
max = max(case_num),
max_date = filing_date[which.max(case_num)]) %>%
arrange (desc(year))
View(json2)
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
# Function to calculate the total request pulls, case num range, durations, and timestamp min/max
calculate_summary <- function(set) {
#set=sets[1][[1]]
session_resets <- set %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
filter (between(timestamp, min(set$timestamp, na.rm=T), max(set$timestamp, na.rm=T))) %>%
filter (str_detect (case_num, "ASP.NET_SessionId")) %>%
nrow() + 1 #+1 because initial setup is before first timestamp -- 3 pulls for each session reset
if (min(set$timestamp) > ymd("2025-01-16")){
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow() * 2)  # Add 1 for each additional case result + 1 request to go back to search list
(set %>% filter(str_detect(note, "NO CASES")) %>% nrow() * 1)  # Add 1 for GET request back from No cases error page.
} else {
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow()) # Add 1 for each additional case result
}
case_range <- range(set$case_num, na.rm = TRUE)
case_diff <- diff(case_range)  # Calculate the difference between max and min
# Calculate the min and max of timestamp
timestamp_min <- min(set$timestamp, na.rm = TRUE)
timestamp_max <- max(set$timestamp, na.rm = TRUE)
durations <- list(
median_duration = median(set$duration, na.rm = TRUE),
mean_duration = round(mean(set$duration, na.rm = TRUE), 2)
)
#this_ip <- toString (unique(set$ip))
#this_gateway <- toString(unique(set$gateway))
#sometimes it pulls in a few wrong IP addresses or gateway, if screengrab didn't work. This chooses the one with the most.
this_ip <- set %>% count (ip, sort=T) %>% head (1) %>% pull (ip)
this_gateway <- set %>% count (gateway, sort=T) %>% head (1) %>% pull (gateway)
this_comp <- set %>% count (comp, sort=T) %>% head (1) %>% pull (comp)
tibble(
pulls = pulls,
sessions = session_resets,
total = pulls + (sessions*3),
min_case = case_range[1],
max_case = case_range[2],
diff = case_diff,
median = durations$median_duration,
mean = durations$mean_duration,
start_timestamp = timestamp_min,
end_timestamp = timestamp_max,
ip = this_ip,
gateway = this_gateway,
comp = this_comp
)
}
year = "2001"
#court_records <- read_csv ("/Users/AFast/Documents/python/output_cases.csv")
#court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/250104_output_cases.csv")
court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_cases.csv")
case_actions <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_case_actions.csv")
case_actions %>% count (event_description, sort=T)
court_records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_cases.csv"))
case_actions <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_case_actions.csv"))
joined <- full_join (court_records, case_actions, by=c("case_number"="case_num"))
if (year >= 2004){
indict <- joined %>%
filter (str_detect(event_description, "INDICT")) %>%
mutate (event_date = mdy(event_date)) %>%
arrange (event_date) %>%
select (event_date, everything())
#  summarize (indict_dates = range(event_date))
arrest <- joined %>%
filter (str_detect(event_description, "ARREST"))  %>%
mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
} else {
indict <- joined %>%
filter (str_detect(event_description, "BINDOVER")) %>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ORIGINAL"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date),
count = n())
}
range(court_records$case_number)
#How many cases have been scraped for this year?
as.numeric(substr(max(court_records$case_number), 7, 12))-as.numeric(substr(min(court_records$case_number), 7, 12))
#How many cases, including those with multiples?
n_distinct(court_records$case_number)
court_records %>% count (disposition, sort=T)
charges <- court_records %>%
count (charge_description, sort=T)
court_records %>% count (def_status, sort=T)
court_records %>%
mutate (case_num = substr(case_number, 7, 12)) %>%
summarize (cases_found = n_distinct(case_num),
i_range = range (case_num))
#find sexual assault/rape cases
rape <- court_records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL"))
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (charge_description, sort = T)
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (disposition, sort = T)
#pull these dockets
rape2 <- rape %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha)
#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
#Combine work laptop and personal laptop logs
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work") #%>%
#add line to split between work and personal if no number end of one.
logs_df <- logs_df %>%
add_row (line = paste(ymd_hms(substr(str_split(tail(logs_df$line, 1), " - ")[[1]][1], 1, 19)) + seconds(1), "- Switching VPN combine work and pers"), comp="work")
logs2 <- read_lines("/Users/AFast/Documents/python/output_private.log")
logs_df2 <- tibble(line = logs2) %>%
mutate (comp = "pers")
logs_df <- rbind (logs_df, logs_df2)
#logs_df <- tibble(line = logs3)
# END NEW CODE
#logs <- read_lines("/Users/AFast/Documents/python/cleveland_court_records/output.log")
#READ JUST THIS TO THIS COMPUTER'S LOG
read = FALSE
if (read==TRUE){
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work")
}
logs_df2 <- logs_df %>%
#logs_df2 <- logs3 %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip", "gateway"),
extra = "drop",
fill = "right") %>%
filter (row_number()>16)
#keeping mult results
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (note = case_when (
str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop") ~ case_num,
#case_num %in% c("2100 iterations. Switching VPN", "Stopping loop: 20 consecutive 'NO CASES' encountered.") ~ case_num,
TRUE ~ note)) %>% #"2100 iterations. Switching VPN", note)) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration)) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down")
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE")|str_detect(logs_df3$note, "Switching VPN")|str_detect(logs_df3$note, "Stopping loop"))
#Find changes in IP address to add to breaks vector
gw_numbers2 <- str_extract(logs_df3$gateway, "(?<=GW:United States #)\\d+")
# Convert to numeric and identify changes
gw_numbers_numeric <- as.numeric(gw_numbers2)
#changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) %>% as.integer()
changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) - 1
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, 36338, 37285, 37837, 37859, 65442, nrow(logs_df3) + 1, changes)
breaks <- unique(sort(breaks))
#first row after IP change is getting excluded when two breaks follow each other sequentially -- keep first
breaks <- breaks[c(TRUE, diff(breaks) != 1)]
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
# Keep only elements with more than one row: two ERROR TERMS IN SERVICE in a row results in a 1-row set.
sets <- keep(sets, ~ nrow(.) > 1)
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds) %>%
arrange (desc(end_timestamp)) %>%
mutate (date = ymd(substr(end_timestamp, 1, 10)),
ip = str_squish(str_remove_all(ip, "IP:")),
gateway = str_squish(str_remove_all(gateway, "GW:"))) %>%
mutate (gateway = case_when (
ip=="134.238.200.155" ~ "US West",
ip=="208.127.87.65" ~ "US Southwest",
ip=="134.238.14.19" ~ "India West",
ip=="134.238.39.64" ~ "Australia Southeast",
ip=="208.127.73.48" ~ "US East",
ip=="208.127.67.72" ~ "US East",
ip=="165.1.204.206" ~ "US Northeast",
ip=="165.1.204.207" ~ "US Northeast",
ip=="130.41.15.98" ~ "??",
is.na(gateway) ~ "Unknown",
TRUE ~ gateway
))
# View the summary
#view(summary)
#session resets
sessions <- logs_df2 %>%
filter (str_detect(case_num, "Name: ASP.NET_SessionId, Value: ")) %>%
mutate (session_id = str_remove_all(case_num, "Name: ASP.NET_SessionId, Value: "))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp)) %>%
select (-case_num)
#recache times
recaching <- logs_df2 %>%
filter (str_detect(case_num, "^Cache expired"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
summary2 <- summary %>%
filter (date == Sys.Date()) %>%
arrange (comp)
view(summary2)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary4 <- summary3 %>% group_by (date) %>% summarize (total=sum(cases))
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases in", year, ":", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2001 cases:", head(summary2$min_case[summary2$comp=="work"],1)-396966, "??"))
print(paste("remaining 2002 cases:", head(summary2$min_case[summary2$comp=="pers"],1)-419392))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
library (sf)
library (geojsonio)
library(smoothr)
library(dplyr)
timeframes <- c("24h", "48h", "72h", "season")
setwd("~/Documents/GitHub/snowfall-accumulation")
tictoc::tic()
lapply(4, function (x){
#x=3
timeframe <- timeframes[[x]]
this_shapefile <- st_read (paste0("python/", timeframe, "_snowfall.shp")) %>%
st_make_valid () %>%
rename (accumulation = DN) %>%
filter (accumulation > 0) %>%
mutate (accumulation = case_when (
accumulation == 1 ~ 0.1,
TRUE ~ accumulation - 1),
area = st_area(geometry))
smoothed_polygons <- smooth(this_shapefile, method = "ksmooth")
smoothed_polygons2 <- smoothed_polygons %>%
st_make_valid()
smoothed_polygons3 <- st_transform(smoothed_polygons2, crs = 5070)
#buffer single pixels at 1300
singlepixels <- smoothed_polygons3 %>%
filter (as.numeric(area) < 20000000) %>%
st_buffer (1200)
#buffer multipixel polygons s at 1500 to fill in gaps
multipixels <- smoothed_polygons3 %>%
filter (as.numeric(area) >= 20000000) %>%
st_buffer (1800)
#rejoin two types
smoothed_polygons4 <- rbind (multipixels, singlepixels) %>%
arrange (desc(area))
#smoothed_polygons4 <- st_buffer (smoothed_polygons3, 1300)
smoothed_polygons5 <- st_transform(smoothed_polygons4, crs = 4326) %>%
arrange(desc(area))
smoothed_polygons6 <- smoothed_polygons5 %>%
st_simplify(dTolerance = 500) %>%
st_make_valid() %>%
arrange(desc(area), desc(accumulation)) #%>%
file3 <- paste0("outputs/", timeframe, "/", timeframe, "_inches_snow_accumulation_latest.json")
geojsonio::topojson_write(smoothed_polygons6,
file = file3,
object_name = "snowfall",
overwrite = TRUE)
})
tictoc::toc()
332/60
#STARS SEASON AND THEN SMOOTH
raster2vector_season <- function() {
#which library to use? stars::read_stars() or terra::rast() -- read_stars() takes longer but is truer to map on NOAA site.
version = "stars"
## construct the URL dynamically
# first get current year
current_year <- as.numeric(format(Sys.Date(), "%Y"))
# determine the start of the season (oct 1 of the current or previous year)
season_start_year <- if (format(Sys.Date(), "%m") < "10") {
current_year - 1  # ff it's before oct, use the previous year
} else {
current_year  # otherwise, use the current year
}
season_start <- paste0(season_start_year, "093012")  # october 1, 12 UTC
# define the end of the season as the current date
season_end <- paste0(format(Sys.Date(), "%Y%m%d"), "12") # hour fixed, no "00" files
# connect all this for seasonal URL
path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date(), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_", season_end, ".tif")
if (version=="terra"){
r <- try(rast(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one day behind the version just tried (1/28/25 instead of 1/29/25, etc.)
new_path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date()-days(1), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_",
format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
r <- rast(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12")
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", season_end)
}
print (paste("Using column:", column_name))
# convert to polygons with rounded raster values
r_poly <- as.polygons(r,
round=TRUE,
digits=1, #round to nearest 1" (will lose 0.01-0.49" to 0)
aggregate=TRUE) #false produces separate features for all with same accumulation value.
}
if (version=="stars"){
r <- try(read_stars(path_to_raster), silent = TRUE)
# Check if path_to_raster hits 404 error because it hasn't been created yet.
if (inherits(r, "try-error")) {
#Create path to one day behind the version just tried (1/28/25 instead of 1/29/25, etc.)
new_path_to_raster <- paste0(
"https://www.nohrsc.noaa.gov/snowfall/data/",
format(Sys.Date()-days(1), "%Y%m"),
"/sfav2_CONUS_", season_start, "_to_",
format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
print(paste(substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster)), "is unavailable."))
print(paste("Now trying", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
r <- read_stars(new_path_to_raster)
print(paste("Pulled", substr(new_path_to_raster, nchar(new_path_to_raster) - 39, nchar(new_path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", format(ymd(substr(season_end, 1, 8))-days(1), "%Y%m%d"), "12", ".tif")
} else {
print(paste("Pulled", substr(path_to_raster, nchar(path_to_raster) - 39, nchar(path_to_raster))))
column_name <- paste0("sfav2_CONUS_", season_start, "_to_", season_end, ".tif")
}
r_poly <- st_as_sf(r, merge = TRUE)  # Merge adjacent areas of same value
}
# convert raster polygons to sf
r_poly2 <- st_as_sf(r_poly) %>%
filter (get(column_name) > 0) %>% #remove NAs and 0
rename (accumulation = column_name)
r_poly3 <- r_poly2 %>%
mutate (accumulation = case_when(
#accumulation< 0 ~ -1,
#accumulation == 0 ~ 0,
between(accumulation, 0, 1) ~ 1, #round 0.1-0.4 up to 1
TRUE ~ round(accumulation)))
#check <- r_poly2 %>% st_drop_geometry()
r_poly4 <- r_poly3 %>%
group_by (accumulation) %>%
summarize (geometry=st_union(geometry)) %>%
st_make_valid()
return(r_poly4)
}
system.time( #202s for stars
snow_list <- raster2vector_season()
)
library(stars)
system.time( #202s for stars
snow_list <- raster2vector_season()
)
