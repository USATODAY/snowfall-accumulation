#case_num %in% c("2100 iterations. Switching VPN", "Stopping loop: 20 consecutive 'NO CASES' encountered.") ~ case_num,
TRUE ~ note)) %>% #"2100 iterations. Switching VPN", note)) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration)) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down")
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE")|str_detect(logs_df3$note, "Switching VPN")|str_detect(logs_df3$note, "Stopping loop"))
#Find changes in IP address to add to breaks vector
gw_numbers2 <- str_extract(logs_df3$gateway, "(?<=GW:United States #)\\d+")
# Convert to numeric and identify changes
gw_numbers_numeric <- as.numeric(gw_numbers2)
#changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) %>% as.integer()
changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) - 1
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, 36338, 37285, 37837, 37859, 65442, nrow(logs_df3) + 1, changes)
breaks <- unique(sort(breaks))
#first row after IP change is getting excluded when two breaks follow each other sequentially -- keep first
breaks <- breaks[c(TRUE, diff(breaks) != 1)]
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
# Keep only elements with more than one row: two ERROR TERMS IN SERVICE in a row results in a 1-row set.
sets <- keep(sets, ~ nrow(.) > 1)
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds) %>%
arrange (desc(end_timestamp)) %>%
mutate (date = ymd(substr(end_timestamp, 1, 10)),
ip = str_squish(str_remove_all(ip, "IP:")),
gateway = str_squish(str_remove_all(gateway, "GW:"))) %>%
mutate (gateway = case_when (
ip=="134.238.200.155" ~ "US West",
ip=="208.127.87.65" ~ "US Southwest",
ip=="134.238.14.19" ~ "India West",
ip=="134.238.39.64" ~ "Australia Southeast",
ip=="208.127.73.48" ~ "US East",
ip=="208.127.67.72" ~ "US East",
ip=="165.1.204.206" ~ "US Northeast",
ip=="165.1.204.207" ~ "US Northeast",
ip=="130.41.15.98" ~ "??",
is.na(gateway) ~ "Unknown",
TRUE ~ gateway
))
# Function to calculate the total request pulls, case num range, durations, and timestamp min/max
calculate_summary <- function(set) {
#set=sets[1][[1]]
session_resets <- logs_df2 %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
filter (between(timestamp, min(set$timestamp, na.rm=T), max(set$timestamp, na.rm=T))) %>%
filter (str_detect (case_num, "ASP.NET_SessionId")) %>%
nrow() + 1 #+1 because initial setup is before first timestamp -- 3 pulls for each session reset
if (min(set$timestamp) > ymd("2025-01-16")){
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow() * 2)  # Add 1 for each additional case result + 1 request to go back to search list
(set %>% filter(str_detect(note, "NO CASES")) %>% nrow() * 1)  # Add 1 for GET request back from No cases error page.
} else {
pulls = (set %>% filter(is.na(mult_num)) %>% nrow() * 2) +  # Each iteration with a single case has two request pulls
(set %>% filter(mult_num == 0) %>% nrow() * 3) +  # Each iteration with multiple cases has two request pulls + 1 for first result
(set %>% filter(mult_num > 0) %>% nrow()) # Add 1 for each additional case result
}
case_range <- range(set$case_num, na.rm = TRUE)
case_diff <- diff(case_range)  # Calculate the difference between max and min
# Calculate the min and max of timestamp
timestamp_min <- min(set$timestamp, na.rm = TRUE)
timestamp_max <- max(set$timestamp, na.rm = TRUE)
durations <- list(
median_duration = median(set$duration, na.rm = TRUE),
mean_duration = round(mean(set$duration, na.rm = TRUE), 2)
)
#this_ip <- toString (unique(set$ip))
#this_gateway <- toString(unique(set$gateway))
#sometimes it pulls in a few wrong IP addresses or gateway, if screengrab didn't work. This chooses the one with the most.
this_ip <- set %>% count (ip, sort=T) %>% head (1) %>% pull (ip)
this_gateway <- set %>% count (gateway, sort=T) %>% head (1) %>% pull (gateway)
this_comp <- set %>% count (comp, sort=T) %>% head (1) %>% pull (comp)
tibble(
pulls = pulls,
sessions = session_resets,
total = pulls + (sessions*3),
min_case = case_range[1],
max_case = case_range[2],
diff = case_diff,
median = durations$median_duration,
mean = durations$mean_duration,
start_timestamp = timestamp_min,
end_timestamp = timestamp_max,
ip = this_ip,
gateway = this_gateway,
comp = this_comp
)
}
#Combine work laptop and personal laptop logs
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work") #%>%
#add line to split between work and personal if no number end of one.
logs_df <- logs_df %>%
add_row (line = paste(ymd_hms(substr(str_split(tail(logs_df$line, 1), " - ")[[1]][1], 1, 19)) + seconds(1), "- Switching VPN combine work and pers"), comp="work")
logs2 <- read_lines("/Users/AFast/Documents/python/output_private.log")
logs_df2 <- tibble(line = logs2) %>%
mutate (comp = "pers")
logs_df <- rbind (logs_df, logs_df2)
#logs_df <- tibble(line = logs3)
# END NEW CODE
#logs <- read_lines("/Users/AFast/Documents/python/cleveland_court_records/output.log")
#READ JUST THIS TO THIS COMPUTER'S LOG
read = FALSE
if (read==TRUE){
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work")
}
logs_df2 <- logs_df %>%
#logs_df2 <- logs3 %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip", "gateway"),
extra = "drop",
fill = "right") %>%
filter (row_number()>16)
#keeping mult results
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (note = case_when (
str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop") ~ case_num,
#case_num %in% c("2100 iterations. Switching VPN", "Stopping loop: 20 consecutive 'NO CASES' encountered.") ~ case_num,
TRUE ~ note)) %>% #"2100 iterations. Switching VPN", note)) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration)) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down")
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE")|str_detect(logs_df3$note, "Switching VPN")|str_detect(logs_df3$note, "Stopping loop"))
#Find changes in IP address to add to breaks vector
gw_numbers2 <- str_extract(logs_df3$gateway, "(?<=GW:United States #)\\d+")
# Convert to numeric and identify changes
gw_numbers_numeric <- as.numeric(gw_numbers2)
#changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) %>% as.integer()
changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) - 1
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, 36338, 37285, 37837, 37859, 65442, nrow(logs_df3) + 1, changes)
breaks <- unique(sort(breaks))
#first row after IP change is getting excluded when two breaks follow each other sequentially -- keep first
breaks <- breaks[c(TRUE, diff(breaks) != 1)]
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
# Keep only elements with more than one row: two ERROR TERMS IN SERVICE in a row results in a 1-row set.
sets <- keep(sets, ~ nrow(.) > 1)
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds) %>%
arrange (desc(end_timestamp)) %>%
mutate (date = ymd(substr(end_timestamp, 1, 10)),
ip = str_squish(str_remove_all(ip, "IP:")),
gateway = str_squish(str_remove_all(gateway, "GW:"))) %>%
mutate (gateway = case_when (
ip=="134.238.200.155" ~ "US West",
ip=="208.127.87.65" ~ "US Southwest",
ip=="134.238.14.19" ~ "India West",
ip=="134.238.39.64" ~ "Australia Southeast",
ip=="208.127.73.48" ~ "US East",
ip=="208.127.67.72" ~ "US East",
ip=="165.1.204.206" ~ "US Northeast",
ip=="165.1.204.207" ~ "US Northeast",
ip=="130.41.15.98" ~ "??",
is.na(gateway) ~ "Unknown",
TRUE ~ gateway
))
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds) %>%
arrange (desc(end_timestamp)) %>%
mutate (date = ymd(substr(end_timestamp, 1, 10)),
ip = str_squish(str_remove_all(ip, "IP:")),
gateway = str_squish(str_remove_all(gateway, "GW:"))) %>%
mutate (gateway = case_when (
ip=="134.238.200.155" ~ "US West",
ip=="208.127.87.65" ~ "US Southwest",
ip=="134.238.14.19" ~ "India West",
ip=="134.238.39.64" ~ "Australia Southeast",
ip=="208.127.73.48" ~ "US East",
ip=="208.127.67.72" ~ "US East",
ip=="165.1.204.206" ~ "US Northeast",
ip=="165.1.204.207" ~ "US Northeast",
ip=="130.41.15.98" ~ "??",
is.na(gateway) ~ "Unknown",
TRUE ~ gateway
))
view(summary)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2004 cases:", summary$min_case[1]-448074))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
library(tidyverse)
library(rvest)
library(httr)
library(janitor)
year = "2004"
#court_records <- read_csv ("/Users/AFast/Documents/python/output_cases.csv")
#court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/250104_output_cases.csv")
court_records <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_cases.csv")
case_actions <- read_csv ("/Users/AFast/Documents/python/cleveland_court_records/2017/2017_output_case_actions.csv")
case_actions %>% count (event_description, sort=T)
court_records <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_cases.csv"))
case_actions <- read_csv (paste0("/Users/AFast/Documents/python/cleveland_court_records/", year, "/", year, "_output_case_actions.csv"))
joined <- full_join (court_records, case_actions, by=c("case_number"="case_num"))
indict <- joined %>%
filter (str_detect(event_description, "INDICT")) %>%
mutate (event_date = mdy(event_date)) %>%
arrange (event_date) %>%
select (event_date, everything())
#  summarize (indict_dates = range(event_date))
arrest <- joined %>%
filter (str_detect(event_description, "ARREST"))  %>%
mutate (event_date = mdy(event_date))  %>%
arrange (event_date) %>%
select (event_date, everything()) %>%
count (month = substr(event_date, 1,7))
#  summarize (arrest_dates = range(event_date))
arrest2 <- joined %>%
filter (str_detect(event_description, "ARREST"))%>%
mutate (event_date = mdy(event_date),
case_group = substr(case_number, 7,10)) %>%
group_by (case_group) %>%
summarize (median = median(event_date),
min = min(event_date),
max = max(event_date))
range(court_records$case_number)
#How many cases have been scraped for this year?
as.numeric(substr(max(court_records$case_number), 7, 12))-as.numeric(substr(min(court_records$case_number), 7, 12))
#How many cases, including those with multiples?
n_distinct(court_records$case_number)
court_records %>% count (disposition, sort=T)
charges <- court_records %>%
count (charge_description, sort=T)
court_records %>% count (def_status, sort=T)
court_records %>%
mutate (case_num = substr(case_number, 7, 12)) %>%
summarize (cases_found = n_distinct(case_num),
i_range = range (case_num))
#find sexual assault/rape cases
rape <- court_records %>%
filter (str_detect(charge_description, "SEX|RAPE")) %>%
filter (!str_detect(charge_description, "SEXUAL CONDUCT WITH AN ANIMAL"))
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (charge_description, sort = T)
rape %>%
#group_by (def_number) %>%
#summarize (charges = n())
count (disposition, sort = T)
#pull these dockets
rape2 <- rape %>%
separate (case_number, into=c("trash", "trash2", "num", "alpha"), sep="-") %>%
distinct (num, alpha)
#2 repeats
n_distinct (rape2$num)
print(paste("most recent arrests scraped:", arrest2$median[1]))
n_distinct(court_records$case_number)
#Combine work laptop and personal laptop logs
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work") #%>%
#add line to split between work and personal if no number end of one.
logs_df <- logs_df %>%
add_row (line = paste(ymd_hms(substr(str_split(tail(logs_df$line, 1), " - ")[[1]][1], 1, 19)) + seconds(1), "- Switching VPN combine work and pers"), comp="work")
logs2 <- read_lines("/Users/AFast/Documents/python/output_private.log")
logs_df2 <- tibble(line = logs2) %>%
mutate (comp = "pers")
logs_df <- rbind (logs_df, logs_df2)
#logs_df <- tibble(line = logs3)
# END NEW CODE
#logs <- read_lines("/Users/AFast/Documents/python/cleveland_court_records/output.log")
#READ JUST THIS TO THIS COMPUTER'S LOG
read = FALSE
if (read==TRUE){
logs <- read_lines("/Users/AFast/Documents/python/output.log")
logs_df <- tibble(line = logs) %>%
mutate (comp = "work")
}
logs_df2 <- logs_df %>%
#logs_df2 <- logs3 %>%
separate (line, sep=" - ", into=c("timestamp", "case_num", "note", "ip", "gateway"),
extra = "drop",
fill = "right") %>%
filter (row_number()>16)
#keeping mult results
logs_df3 <- logs_df2 %>%
filter (str_detect (case_num, "^\\d")|str_detect(str_squish(case_num), "^Process")|str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop")) %>%
mutate (case_num = str_remove_all(case_num, "Processing case: |Processed case: |Processing case number: ")) %>%
separate (case_num, sep="-", into=c("case_num", "mult_num")) %>%
mutate (note = case_when (
str_detect(case_num, "Switching VPN")|str_detect(case_num, "Stopping loop") ~ case_num,
#case_num %in% c("2100 iterations. Switching VPN", "Stopping loop: 20 consecutive 'NO CASES' encountered.") ~ case_num,
TRUE ~ note)) %>% #"2100 iterations. Switching VPN", note)) %>%
mutate (case_num = as.numeric(case_num)) %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp),
note = coalesce (note, mult_num)) %>%
#manually remove the large durations from rows right after the error.
mutate (duration = if_else(lag(note=="ERROR TERMS OF SERVICE" | is.na(note)), as.difftime(NA_real_, units = "secs"), duration)) %>%
fill(ip, .direction = "down") %>%
fill(gateway, .direction = "down")
#duration = if_else (lag(str_detect(note, "ERROR TERMS OF SERVICE")), NA_complex_, duration))
#where did I reset VPN?
# Find the breakpoints
breaks <- which(str_detect(logs_df3$note, "ERROR TERMS OF SERVICE")|str_detect(logs_df3$note, "Switching VPN")|str_detect(logs_df3$note, "Stopping loop"))
#Find changes in IP address to add to breaks vector
gw_numbers2 <- str_extract(logs_df3$gateway, "(?<=GW:United States #)\\d+")
# Convert to numeric and identify changes
gw_numbers_numeric <- as.numeric(gw_numbers2)
#changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) %>% as.integer()
changes <- which(c(FALSE, diff(gw_numbers_numeric, na.rm = TRUE) != 0)) - 1
breaks <- c(0, breaks, 10785, 11081, 11383, 15576, 15743, 22149, 25157, 25684, 30185, 36338, 37285, 37837, 37859, 65442, nrow(logs_df3) + 1, changes)
breaks <- unique(sort(breaks))
#first row after IP change is getting excluded when two breaks follow each other sequentially -- keep first
breaks <- breaks[c(TRUE, diff(breaks) != 1)]
# Create subsets for each range
sets <- map(seq_along(breaks)[-length(breaks)], ~ {
logs_df3 %>%
filter(row_number() > breaks[.x] & row_number() <= breaks[.x + 1])
})
# Keep only elements with more than one row: two ERROR TERMS IN SERVICE in a row results in a 1-row set.
sets <- keep(sets, ~ nrow(.) > 1)
medians <- map_dbl(sets, ~ median(.x$duration, na.rm = TRUE))
#tail(sets, 1)[[1]]
no_cases <- logs_df3 %>%
filter (note == "NO CASES")
# Apply the function to each set and combine results into one summary tibble
summary <- map_dfr(sets, calculate_summary) %>%
filter (pulls>0) %>%
mutate (session_duration_seconds = as.numeric(difftime(end_timestamp, start_timestamp, units = "secs")),#
#Format the session duration as HH:MM:SS
session_duration = sprintf("%02d:%02d:%02d",
session_duration_seconds %/% 3600,           # Hours
(session_duration_seconds %% 3600) %/% 60,   # Minutes
session_duration_seconds %% 60),              # Seconds
cases_per_hr = round(diff/session_duration_seconds*3600)) %>%
select (-session_duration_seconds) %>%
arrange (desc(end_timestamp)) %>%
mutate (date = ymd(substr(end_timestamp, 1, 10)),
ip = str_squish(str_remove_all(ip, "IP:")),
gateway = str_squish(str_remove_all(gateway, "GW:"))) %>%
mutate (gateway = case_when (
ip=="134.238.200.155" ~ "US West",
ip=="208.127.87.65" ~ "US Southwest",
ip=="134.238.14.19" ~ "India West",
ip=="134.238.39.64" ~ "Australia Southeast",
ip=="208.127.73.48" ~ "US East",
ip=="208.127.67.72" ~ "US East",
ip=="165.1.204.206" ~ "US Northeast",
ip=="165.1.204.207" ~ "US Northeast",
ip=="130.41.15.98" ~ "??",
is.na(gateway) ~ "Unknown",
TRUE ~ gateway
))
# View the summary
#view(summary)
#session resets
sessions <- logs_df2 %>%
filter (str_detect(case_num, "Name: ASP.NET_SessionId, Value: ")) %>%
mutate (session_id = str_remove_all(case_num, "Name: ASP.NET_SessionId, Value: "))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp)) %>%
select (-case_num)
#recache times
recaching <- logs_df2 %>%
filter (str_detect(case_num, "^Cache expired"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
redirects <- logs_df2 %>%
filter (str_detect(case_num, "redirect"))  %>%
mutate (timestamp = ymd_hms(substr(timestamp,1,19))) %>%
mutate (duration = timestamp - lag(timestamp))
view(summary)
time_left <- (2100-summary$diff[1])*as.numeric(summary$median[1])
cases_done <- 1482
cases_done <- summary %>%
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T),
last_timestamp = max(end_timestamp)) %>%
arrange (desc(date), desc(last_timestamp)) %>% #desc(cases)) %>%
head (1) %>%
pull (cases)
time_left <- (1900-cases_done)*as.numeric(summary$median[1])
# Calculate hours, minutes, and seconds
hours <- floor(time_left / 3600)
minutes <- floor((time_left %% 3600) / 60)
seconds <- time_left %% 60
# Format to HH:MM:SS
formatted_time <- sprintf("%02d:%02d:%02d", hours, minutes, seconds)
#How many were scraped each day?
summary3 <- summary %>%
group_by (date, comp) %>%
summarize (cases = sum(diff, na.rm=T))
#arrange (desc(date))
#arrange (-cases) %>%
#adorn_totals()
summary3 %>%
ggplot (aes(x=date, y=cases, fill=comp, group=comp)) +
geom_bar(stat="identity") +
#geom_smooth(method="lm") +
scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
scale_y_continuous(
breaks = seq(0, max(summary3 %>% group_by(date) %>% summarize (total=sum(cases, na.rm=T)) %>% pull (total)), by = 1000),  # Set custom breaks
labels = scales::comma  # Optional: Format the labels (e.g., with commas)
) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", title = "Cases scraped by date")
#How many were scraped each day per IP address?
summary %>%
filter (date >= (Sys.Date())) %>%#-days(2)))
group_by (date, ip, gateway) %>%
summarize (cases = sum(diff, na.rm=T)) %>%
arrange (desc(date), desc(cases)) %>%
adorn_totals()
print(paste("distinct rape cases this year:", n_distinct (rape2$num)))
print(paste("most recent arrests scraped:", arrest2$median[1]))
print(paste("remaining 2004 cases:", summary$min_case[1]-448074))
print(paste("VPN will reset at approximately", format(Sys.time() + time_left, "%H:%M:%S"), ", from now:", formatted_time))
View(no_cases)
